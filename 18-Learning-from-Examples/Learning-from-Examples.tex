% !TEX TS-program = pdflatexmk

\documentclass[14pt]{beamer}
\input{../slides-preamble}

\title{Learning from Examples}
\date[]{20 Mar 2014}

\begin{document}


\begin{frame}
  \titlepage
\end{frame}

\section{Learning Basics}

\subsection{Learning Functions}

\begin{frame}{Supervised Learning}
\begin{block}{Key Ideas}
\begin{itemize}
\item Examine pairs of inputs and outputs
\item Guess a possible function mapping input to output
\item Predict outputs given new inputs
\end{itemize}
\end{block}
\bigskip
\begin{columns}
\begin{column}<2->{1.1in}
$
\begin{array}{lll}
f(1) & = & 1 \\
f(2) & = & 4 \\
f(3) & = & 9 \\
f(4) & = & 16 \\
f(5) & = & \alt<3->{\alert{25}}{?} \\
\end{array}
$
\end{column}
\begin{column}<4->{3.1in}
\begin{tabular}{lll}
$f$(Romeo and Juliet) & = & Shakespeare \\
$f$(Tom Sawyer)       & = & Twain \\
$f$(Macbeth)          & = & Shakespeare \\
$f$(Huckleberry Finn) & = & Twain \\
$f$(Othello)          & = & \alt<5->{\alert{Shakespeare}}{?} \\
\end{tabular}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Supervised Learning Components}
\begin{block}{Original Function}
An unknown function $f\!: D_f \rightarrow R_f$
\end{block}
\pause
\begin{block}{Training Examples}
Pairs of $(x, f(x))$ where $x \in D_f$ and $f(x) \in R_f$
\end{block}
\pause
\begin{block}{Hypothesis Function}
Some function $h\!: D_f \rightarrow R_f$
\end{block}
\pause
\begin{block}{Learning Goal}
Pick a hypothesis $h$ as close to $f$ as possible
\end{block}
\end{frame}

\begin{frame}{Selecting Hypotheses}
\begin{columns}
\begin{column}{2.1in}
Many hypotheses possible:\\
\bigskip
\only<1>{\includegraphics[height=1.5in]{curve-fitting-1}}%
\only<2>{\includegraphics[height=1.5in]{curve-fitting-2}}%
\only<3>{\includegraphics[height=1.5in]{curve-fitting-3}}%
\only<4>{\includegraphics[height=1.5in]{curve-fitting-4}}%
\only<5->{\includegraphics[height=1.5in]{curve-fitting-5}}%
\end{column}
\begin{column}<6->{2.1in}
\begin{block}{Goals}
\begin{itemize}
\item Maximize number of examples where $h$ agrees with $f$
\item Minimize complexity of $h$ function
\end{itemize}
\end{block}
\bigskip
\uncover<7->{Typically a tradeoff between consistency and simplicity}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Overfitting}
\begin{block}{Definition}
A hypothesis $h$ has \alert{overfit} the data if it uses a function that is more complex than necessary to explain the data
\end{block}
\smallskip
\begin{columns}[t]
\begin{column}<2->{1in}
Examples: \\
\medskip
$
\begin{array}{lll}
f(1) & = & 2 \\
f(2) & = & 4 \\
f(3) & = & 6 \\
\end{array}
$
\end{column}
\begin{column}<3->{3in}
Hypotheses: \\
\medskip
$
\begin{array}{lll}
h_1(x) & = & 2x \\
\uncover<4->{h_2(x) & = & \alert<5->{x^3 - 6x^2 + 13x - 6}} \\
\end{array}
$ \\
\medskip
\uncover<5->{$h_2$ has probably overfit the data} \\
\uncover<6->{\ldots\ unless $f(x) = x^3 - 6x^2 + 13x - 6$}
\end{column}
\end{columns}
\begin{block}<7->{Lesson}
Use as many parameters as we expect $f$ to have, no more
\end{block}
\end{frame}

\subsection{Problem Formulation}

\begin{frame}{Problem Formulation}
\begin{block}{Defining a Machine Learning Problem}
Describe the function $f\!: D_f \rightarrow R_f$
\begin{itemize}
\item What is $D_f$?
\item What is $R_f$?
\end{itemize}
\end{block}
\pause
\begin{columns}
\begin{column}{2in}
\begin{block}{Ex: Income Prediction}
\begin{itemize}
\pause
\item $D_f$ = \pause humans
\pause
\item $R_f$ = \pause $\mathbb{R}$ (yearly income)
\end{itemize}
\end{block}
\end{column}
\pause
\begin{column}{2in}
\begin{block}{Ex: Cancer Diagnosis}
\begin{itemize}
\pause
\item $D_f$ = \pause animals
\pause
\item $R_f$ = \pause $\{\textit{true},\textit{false}\}$
\end{itemize}
\end{block}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Decomposing Domain Objects}
\begin{columns}
\begin{column}{1.6in}
Smoker identification: \\
\smallskip
\begin{tabular}{lll}
$f$(John)  & = & \textit{true} \\
$f$(Mary)  & = & \textit{false} \\
$f$(Frank) & = & \textit{false} \\
$f$(Sally) & = & ?
\end{tabular}
\end{column}
\pause
\begin{column}{2.6in}
\begin{block}{But name is insufficient!}
Need to know, e.g.
\begin{itemize}
\item smells like cigarettes?
\item has yellow teeth?
\end{itemize}
\end{block}
\end{column}
\end{columns}
\bigskip
\pause
\begin{block}{Definition}
\alert{Attributes} or \alert{features} are the components of a domain object believed to be important for learning the function $f$
\end{block}
\end{frame}

\begin{frame}{Part of Speech Tagging Example}
\begin{tabular}{llllll}
\keyword{Input}  & \em John & \em broke & \em the & \em red & \em lamp \\
\pause
\keyword{Output} & \sc Noun & \sc Verb  & \sc Det & \sc Adj & \sc Noun \\
\end{tabular}
\pause
\begin{block}{Function Description}
\begin{itemize}
\pause
\item $D_f$ = \pause $\{\textit{a},\textit{aardvark},\textit{abacus},\textit{abalone},\ldots\}$
\pause
\item $R_f$ = \pause $\{\textsc{Noun},\textsc{Verb},\textsc{Adj},\textsc{Adv},\textsc{Det},\ldots\}$
\end{itemize}
\end{block}
\medskip
\pause
\begin{columns}
\begin{column}{2in}
$f$(\textit{bark}) = \textsc{Noun}? \textsc{Verb}?
\end{column}
\begin{column}{2in}
\emph{\ldots the \textbf{bark} of the tree \ldots} \\
\emph{\ldots heard the dog \textbf{bark} \ldots} \\
\end{column}
\end{columns}
\pause
\begin{block}{Feature Representation}
$
\begin{array}{lll}
f([w_{0}\!=\!\textit{bark}, w_{-1}\!=\!\textit{the}]) & = & \textsc{Noun} \\
f([w_{0}\!=\!\textit{bark}, w_{-1}\!=\!\textit{dog}]) & = & \textsc{Verb}
\end{array}
$
\end{block}
\end{frame}

\begin{frame}{Named Entity Recognition Exercise}
\begin{block}{Definition}
A \alert{named entity recognition} program find spans of words that are people, locations, organizations, etc.\\
\medskip
\begin{tabular}{llllll}
\keyword{Input}  & Bill works for Microsoft Corporation\\
\keyword{Output} & $[_{\textsc{\scriptsize Per}}$ Bill$]$ works for
               $[_{\textsc{\scriptsize Org}}$ Microsoft Corporation$]$ \\
\end{tabular}
\end{block}
\pause
\begin{block}{Exercise}
Named entity recognition as supervised learning:
\begin{itemize}
\item Describe the function domain
\item Describe the function range
\item Describe the feature space
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{One Named Entity Recognition Approach}
\begin{block}{Function Description}
\begin{itemize}
\item $D_f$ = \pause $\{\textit{a},\textit{aardvark},\textit{abacus},\textit{abalone},\ldots\}$
\item $R_f$ = \pause $\{\textsc{B-Per},\textsc{I-Per},\textsc{B-Org},\textsc{I-Org},\ldots,\textsc{O}\}$
\end{itemize}
\end{block}
\pause
$
\begin{array}{lll}
f(\textit{Bill})        & = & \textsc{B-Per} \\
f(\textit{works})       & = & \textsc{O} \\
f(\textit{for})         & = & \textsc{O} \\
f(\textit{Microsoft})   & = & \textsc{B-Org} \\
f(\textit{Corporation}) & = & \textsc{I-Org} \\
\end{array}
$
\pause
\begin{block}{Typical Features}
\begin{tabular}{p{1.8in}p{1.8in}}
Word itself     & Capitalization \\
Preceding label & First in sentence? \\
\end{tabular}
\end{block}
\end{frame}

\subsection{Learning Algorithms}

\begin{frame}{Learning Supervised Models}
After problem formulation, we apply a \alert{learning algorithm} \\
\medskip
\begin{block}{Input}
A set of $(x, f(x))$ training examples
\end{block}
\pause
\begin{block}{Output}
A function $h\!: D_f \rightarrow R_f$
\end{block}
\pause
\begin{block}{Goal}
Pick an $h$ that:
\begin{itemize}
\item is consistent with the training examples
\item does not overfit the data (i.e. is not overly complex)
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Evaluating Learning Algorithms}
\begin{enumerate}
\item Train learning algorithm on examples $E_{\textit{\scriptsize train}}$
\item Learning algorithm produces a hypothesis $h$
\item Test hypothesis on new examples $E_{\textit{\scriptsize test}}$ \uncover<2->{- \alert{Don't peek!}}
\end{enumerate}
\begin{columns}
\begin{column}<3->{2in}
Train:
$
\begin{array}[t]{ll|l}
F_1 & F_2 & \textit{Class} \\
\hline
0   & a   & \textit{true} \\
0   & b   & \textit{false} \\
0   & c   & \textit{false} \\
1   & b   & \textit{false} \\
\end{array}
$
\end{column}
\begin{column}<6->{2in}
Test:
$
\begin{array}[t]{ll|l}
F_1 & F_2 & \textit{Class} \\
\hline
1   & a   & \textit{true} \\
0   & b   & \textit{false} \\
0   & c   & \textit{false} \\
1   & c   & \textit{true} \\
\end{array}
$
\end{column}
\end{columns}
\begin{columns}
\begin{column}<4->{2in}
\begin{block}{Algorithm 1}
Always classify as \textit{false}
\\
\uncover<7->{\tab Performance:} \uncover<8->{$\frac{2}{4} = 0.5$}
\end{block}
\end{column}
\begin{column}<5->{2in}
\begin{block}{Algorithm 2}
If $a$ then \textit{true}, else \textit{false}
\\
\uncover<7->{\tab Performance:} \uncover<9->{$\frac{3}{4} = 0.75$}
\end{block}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Learning Curves}
\begin{itemize}
\item Train on increasing fractions of training data
\item Test each of the resulting hypotheses on the test data
\end{itemize}
\pause
\begin{center}
\includegraphics[height=2in]{restaurant-dtl-curve}
\end{center}
\end{frame}

\section{Decision Trees}

\subsection{Decision Tree Basics}

\begin{frame}{Decision Trees}
\begin{columns}
\begin{column}{2.7in}
\centering
Should I play golf today? \\
\bigskip
\includegraphics[width=2.7in]{play_golf}
\end{column}
\begin{column}{1.5in}
\begin{block}{Function}
$D_f$ = days \\
$R_f$ = $\{\textit{Yes}, \textit{No}\}$
\end{block}
\bigskip
\begin{block}{Features}
\begin{itemize}
\item Weather
\item Humidity
\item Wind
\end{itemize}
\end{block}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Decision Trees as Functions}
\begin{columns}[t]
\begin{column}{2in}
$f(X, Y) = X \textit{ xor } Y$ \\[1em]
\includegraphics[scale=.75]{xor}
\end{column}
\begin{column}{2in}
$f(A, B, C) = (A \land B) \lor \lnot C$ \\[1em]
\includegraphics[scale=.75]{abc}
\end{column}
\end{columns}
\end{frame}

\subsection{Decision Tree Learning}

\begin{frame}{Learning Decision Trees}
\begin{enumerate}
\item Select a feature $F$ for the node
\item For each value of $F$, create a child node
\item Sort training examples into child nodes
\item If examples are sorted perfectly, terminate
\item Else, repeat the process for each child node
\end{enumerate}
\begin{columns}
\begin{column}<2->{1.7in}
$
\begin{array}{cc|c}
X          &  Y          & f(X, Y) \\
\hline
\textit{true}  & \textit{true}   & \textit{true} \\
\textit{true}  & \textit{false}  & \textit{false} \\
\textit{false} & \textit{true}   & \textit{true} \\
\textit{false} & \textit{false}  & \textit{true} \\
\end{array}
$
\end{column}
\begin{column}{2.6in}
\only<-2>{\invisible{\includegraphics[scale=.75]{impl}}}%
\only<3>{\includegraphics[scale=.75]{impl-1}}%
\only<4>{\includegraphics[scale=.75]{impl-2}}%
\only<5>{\includegraphics[scale=.75]{impl-3}}%
\only<6>{\includegraphics[scale=.75]{impl-4}}%
\only<7>{\includegraphics[scale=.75]{impl-5}}%
\only<8>{\includegraphics[scale=.75]{impl-6}}%
\only<9>{\includegraphics[scale=.75]{impl-7}}%
\end{column}
\end{columns}
\end{frame}

\begin{frame}[label=decision-tree-exercise]{Decision Tree Exercise}
\begin{columns}
\begin{column}{1.8in}
Build a decision tree for: \\
\bigskip
$
\begin{array}{ll|l}
F_1 & F_2 & \textit{Class} \\
\hline
0   & a   & \textit{true} \\
0   & b   & \textit{false} \\
1   & c   & \textit{true} \\
0   & c   & \textit{false} \\
1   & b   & \textit{false} \\
1   & a   & \textit{true} \\
\end{array}
$
\end{column}
\pause
\begin{column}{1.8in}
Two possible solutions: \\
\centering
\bigskip
\includegraphics[scale=.6]{exercise-1} \\
\bigskip
\includegraphics[scale=.6]{exercise-2}
\end{column}
\end{columns}
\end{frame}

\subsection{Features and Information}

\begin{frame}{Selecting Features for Decision Trees}
\begin{block}{Feature Selection Order}
\begin{itemize}
\item Different orders result in different trees
\item ``Good'' features should be used before ``poor'' ones
\end{itemize}
\end{block}
\pause
\begin{block}{What is a ``good'' feature?}
One whose values predict the class labels \\
\end{block}
\pause
\medskip
\begin{columns}
\begin{column}{1.2in}
$
\begin{array}{lll}
F_1 & F_2 & \textit{Class} \\
\hline
a & 0 & \textit{true} \\
a & 1 & \textit{true} \\
b & 0 & \textit{false} \\
b & 1 & \textit{false} \\
\end{array}
$
\end{column}
\pause
\begin{column}{1.8in}
$F_1$ is a \alert{good} feature \\
\bigskip
$F_2$ is a \alert{poor} feature
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Information}
\begin{block}{Good features provide more information}
Information can be quantified in terms of bits
\end{block}
\pause
Task: Encode \texttt{abacabad} using as few bits as possible
\pause
\medskip
\begin{columns}[t]
\begin{column}{1.8in}
Simple Encoding:
\begin{tabular}{ll}
a & 00 \\
b & 01 \\
c & 10 \\
d & 11 \\
\end{tabular} \\
\pause
\medskip
\texttt{abacabad} $\rightarrow$ 16 bits \texttt{0001001000010011}
\end{column}
\pause
\begin{column}{1.8in}
Using Probability:
\begin{tabular}{ll}
a & 0 \\
b & 10 \\
c & 110 \\
d & 111 \\
\end{tabular} \\
\pause
\medskip
\texttt{abacabad} $\rightarrow$ 14 bits \texttt{01001100100111}
\end{column}
\end{columns}
\pause
\bigskip
\alert{More likely values can be encoded in fewer bits!}
\end{frame}

\begin{frame}{Entropy}
\begin{block}{Definition}
The \alert{entropy} of a random variable $X$ is: \\
\smallskip
\tab\tab$H(X) = -\sum\limits_{x \in X}P(x)\log_2 P(x)$
\end{block}
\pause
\begin{block}{Bit-based Interpretation}
Smallest number of bits that can encode a stream of values from $X$'s distribution
\end{block}
\pause
\begin{block}{Intuitions}
\begin{itemize}
\item High entropy $\rightarrow$ boring (e.g. uniform) distribution
\item Low entropy $\rightarrow$ interesting distribution
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Entropy}
\begin{columns}
\begin{column}{1.1in}
\begin{tabular}{ll}
X       & Y   \\
\hline
math    & yes \\
history & no  \\
cs      & yes \\
math    & no  \\
math    & no  \\
cs      & yes \\
history & no  \\
math    & yes \\
\end{tabular}
\end{column}
\begin{column}{3.1in}
\pause
$
\extrarowheight=2pt
\begin{array}{lll}
H(X)        & = & \pause -\sum\limits_{x \in X}P(x)\log_2 P(x) \\
     \pause & = & -P(\textit{math})\log_2 P(\textit{math}) \\
            &   & -P(\textit{history})\log_2 P(\textit{history}) \\
            &   & -P(\textit{cs})\log_2 P(\textit{cs}) \\
     \pause & = & -\frac{1}{2}\log_2\frac{1}{2} 
                  -\frac{1}{4}\log_2\frac{1}{4}
                  -\frac{1}{4}\log_2\frac{1}{4} \\
     \pause & = & -\frac{1}{2}(-1) - \frac{1}{4}(-2) - \frac{1}{4}(-2) \\
     \pause & = & \frac{1}{2} + \frac{1}{2} + \frac{1}{2} \\
     \pause & = & 1.5 \\
\\
\pause
H(Y)        & = & \pause 1.0 \\
\end{array}
$
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Specific Conditional Entropy}
\begin{columns}
\begin{column}{1.1in}
\begin{tabular}{ll}
X               & Y   \\
\hline
math            & yes \\
history         & no  \\
\alert<4-5>{cs} & \alert<4>{yes} \\
math            & no  \\
math            & no  \\
\alert<4-5>{cs} & \alert<4>{yes} \\
history         & no  \\
math            & yes \\
\end{tabular}
\end{column}
\begin{column}{3.1in}
\begin{block}{Specific Conditional Entropy}
$H(Y|X\!=\!x) = -\sum\limits_{y \in Y}P(y|x)\log_2 P(y|x)$
\end{block}
\small
$
\extrarowheight=2pt
\begin{array}{@{}l@{}l@{}l@{}}
\uncover<2->{H(Y|X\!=\!\textit{cs})}
& \uncover<2->{=}
& \uncover<3->{\alert<4>{-P(\textit{yes}|\textit{cs})\log_2 P(\textit{yes}|\textit{cs})}}
\\
&
& \uncover<3->{\alert<5>{-P(\textit{no}|\textit{cs})\log_2 P(\textit{no}|\textit{cs})}}
\\
& \uncover<4->{=}
& \uncover<4->{\alert<4>{-1 \log_2 1}}\uncover<5->{\alert<5>{-0 \log_2 0}}
\\
& \uncover<6->{=}
& \uncover<6->{-1 \cdot 0 - 0 \cdot \infty}
\\
& \uncover<7->{=}
& \uncover<7->{0} \\
\end{array}
$ \\
\medskip
\uncover<8->{In other words, $X\!=\!\textit{cs}$ is a great predictor of $Y$}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Conditional Entropy}
\begin{columns}
\begin{column}{1.1in}
\begin{tabular}{ll}
X               & Y   \\
\hline
math            & yes \\
history         & no  \\
cs              & yes \\
math            & no  \\
math            & no  \\
cs              & yes \\
history         & no  \\
math            & yes \\
\end{tabular}
\end{column}
\begin{column}{3.1in}
\begin{block}{Conditional Entropy}
$
\begin{array}{lll}
H(Y|X) & = & \sum\limits_{x \in X}P(X\!=\!x)H(Y|X\!=\!x)
\end{array}
$
\end{block}
\pause
\small
Given:
$
\begin{array}[t]{lll}
H(Y|X\!=\!\textit{math})    & = & 1 \\
H(Y|X\!=\!\textit{history}) & = & 0 \\
H(Y|X\!=\!\textit{cs})      & = & 0 \\
\end{array}
$ \\
\smallskip
\pause
$
\extrarowheight=2pt
\begin{array}{@{}l@{\hspace{2pt}}l@{\hspace{2pt}}l@{}}
H(Y|X) & = & \pause P(X\!=\!\textit{math}) H(Y|X\!=\!\textit{math}) + \mbox{}\\
       &   & P(X\!=\!\textit{history}) H(Y|X\!=\!\textit{history}) + \mbox{}\\
       &   & P(X\!=\!\textit{cs}) H(Y|X\!=\!\textit{cs}) \\
\pause & = & \frac{1}{2} \cdot 1 + \frac{1}{4} \cdot 0 + \frac{1}{4} \cdot 0 \pause = \frac{1}{2}
\end{array}
$ \\
\medskip
\pause
In other words, $X$ is a good predictor of $Y$
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Information Gain}
\begin{columns}
\begin{column}{1.1in}
\begin{tabular}{ll}
X               & Y   \\
\hline
math            & yes \\
history         & no  \\
cs              & yes \\
math            & no  \\
math            & no  \\
cs              & yes \\
history         & no  \\
math            & yes \\
\end{tabular}
\end{column}
\begin{column}{3.1in}
\begin{block}{Information Gain}
$IG(Y|X) = H(Y) - H(Y|X)$
\end{block}
\pause
\begin{block}{Intuitive Explanation}
How many bits would it save to know $X$?
\end{block}
\pause
$
\begin{array}[t]{lll}
H(Y|X)  & = & 0.5 \\
H(Y)    & = & 1 \\
\\
\pause
IG(Y|X) & = & \pause 1 - 0.5 = 0.5
\end{array}
$
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Decision Trees and Information Gain}
\textbf{\keyword{Select the feature with the highest information gain}}
\pause
\medskip
\begin{columns}[T]
\begin{column}{0.6in}
\small
$
\begin{array}{@{}l@{\hspace{2pt}}l@{\hspace{2pt}}|@{\hspace{2pt}}l@{}}
F_1 & F_2 & Y \\
\hline
0   & a   & \textsc{t} \\
0   & b   & \textsc{f} \\
1   & c   & \textsc{t} \\
0   & c   & \textsc{f} \\
1   & b   & \textsc{f} \\
1   & a   & \textsc{t} \\
\end{array}
$
\end{column}
\pause
\begin{column}{3.8in}
\small
$
\extrarowheight=2pt
\begin{array}{@{}l@{\hspace{2pt}}l@{\hspace{2pt}}l@{}}
H(Y)               & = & \pause -P(\textsc{t})\log_2 P(\textsc{t}) - P(\textsc{f})\log_2 P(\textsc{f}) \\
\pause
                   & = & -\frac{1}{2}\log_2 \frac{1}{2} - \frac{1}{2}\log_2 \frac{1}{2} \pause = 1 \\
\pause
H(Y|F_1\!\!=\!\!0) & = & \pause -P(\textsc{t}|0)\log_2 P(\textsc{t}|0) - \pause P(\textsc{f}|0)\log_2 P(\textsc{f}|0) \\
\pause
                   & = & - \frac{1}{3}\log_2 \frac{1}{3} - \pause \frac{2}{3}\log_2 \frac{2}{3} \pause = 0.92\\
\pause         
H(Y|F_1\!\!=\!\!1) & = & \pause \ldots = 0.92\\
\pause         
H(Y|F_1)           & = & \pause P(F_1\!\!=\!\!0) H(Y|F_1\!\!=\!\!0) + P(F_1\!\!=\!\!1) H(Y|F_1\!\!=\!\!1) \\
\pause         
                   & = & 0.5 \cdot 0.92 + \pause 0.5 \cdot 0.92 \pause = 0.92 \\
\pause
IG(Y|F_1)          & = & \pause H(Y) - H(Y|F_1) \pause = 1 - 0.92 \pause = 0.08 \\
\pause         
IG(Y|F_2)          & = & \pause \mbox{\alert{ Your turn! }} \pause = 0.67 \\
\end{array}
$ \\
\medskip
\pause
So $F_2$ is a much better feature to split on \hyperlink{decision-tree-exercise<2>}{\beamerbutton{See Example}}
\end{column}
\end{columns}
\end{frame}

\section{Ensemble Learning}

\begin{frame}{Ensemble Learning}
\begin{block}{Key Idea}
Two (or more) classifiers are better than one
\end{block}
\begin{center}
\includegraphics[height=1.8in]{ensemble-expressiveness}
\end{center}
\end{frame}

\subsection{Bagging}

\begin{frame}[<+->]{Bagging}
\begin{block}{Key Ideas}
\begin{itemize}
\item Error is partly due to particular choice of training set
\item Create $m$ similar training sets and learn $m$ models
\end{itemize}
\end{block}
\begin{block}{Bagging (\textbf{B}ootstrap \textbf{agg}regat\textbf{ing})}
Given a training set of $n$ examples:
\begin{enumerate}
\item Generate $m$ new training sets (\alert{bootstrap samples})
\begin{itemize}
\item Each new training set contains $n$ examples
\item Examples selected randomly, with replacement
\end{itemize}
\item Train $m$ models on the $m$ new training sets
\item Classify new examples by taking the majority vote
\end{enumerate}
\end{block}
\end{frame}

\begin{frame}{Bagging Exercise}
\begin{columns}
\begin{column}{2.8in}
\begin{itemize}
\item Generate two bootstrap samples
\item Use indices $[1, 3, 2, 3]$ $[2, 0, 3, 2]$
\item Create a decision tree for each
\item Classify $000$ and $101$
\end{itemize}
\end{column}
\begin{column}{1.2in}
$
\begin{array}{lll|l}
F_1 & F_2 & F_3 & C \\
\hline
0   & 1   & 1   & T \\
0   & 1   & 0   & F \\
0   & 0   & 1   & F \\
1   & 1   & 0   & T
\end{array}
$
\end{column}
\end{columns}
\pause
\begin{columns}
\begin{column}{2in}
\begin{block}{Sample 1}
$
\begin{array}{lll|l}
0   & 1   & 0   & F \\
1   & 1   & 0   & T \\
0   & 0   & 1   & F \\
1   & 1   & 0   & T
\end{array}
$
\begin{tabular}{l}
\includegraphics[height=.8in]{bagging-1}
\end{tabular}
\end{block}
\end{column}
\begin{column}{2in}
\begin{block}{Sample 2}
$
\begin{array}{lll|l}
0   & 0   & 1   & F \\
0   & 1   & 1   & T \\
1   & 1   & 0   & T \\
0   & 0   & 1   & F
\end{array}
$
\begin{tabular}{l}
\includegraphics[height=.8in]{bagging-2}
\end{tabular}
\end{block}
\end{column}
\end{columns}
\begin{center}
$
\begin{array}{lllll}
\textit{classify}(000) & = & \textit{majority}([F, F]) & = & F \\
\textit{classify}(101) & = & \textit{majority}([T, F]) & = & ?
\end{array}
$
\end{center}
\end{frame}

\begin{frame}{Bagging Behavior}
\begin{center}
\includegraphics[scale=0.5]{ozone}
\end{center}
\end{frame}

\begin{frame}{Bagging Properties}
\begin{block}{Complexity}
\begin{itemize}
\pause
\item Training time: \pause $m$ times original
\pause
\item Model size: \pause $m$ times original
\pause
\item Classification time: \pause $m$ times original
\end{itemize}
\end{block}
\pause
\begin{block}{Practical Issues}
\begin{itemize}
\item Can improve performance of unstable learners
\item Typically does not improve stable learners
\end{itemize}
\end{block}
\end{frame}

\subsection{Boosting}

\begin{frame}[<+->]{Boosting}
\begin{block}{Key Ideas}
\begin{itemize}
\item Look at examples the first classifier got wrong
\item Create new models that do better on these
\end{itemize}
\end{block}
\begin{block}{Boosting}
\begin{enumerate}
\item Start with all training samples weighted equally
\item\label{train-step} Train a model (e.g. a single feature \alert{decision stump})
\item\label{weight-step} Increase the weight of all misclassified examples
\item Repeat steps \ref{train-step} and \ref{weight-step} until $m$ models are generated
\end{enumerate}
\end{block}
\end{frame}

\begin{frame}{Boosting Example}
\includegraphics[width=1.4in]{boosting_f1}
\includegraphics[width=1.4in]{boosting_f2}
\includegraphics[width=1.4in]{boosting_f3}
\medskip
\small
\begin{columns}
\begin{column}{1.5in}
$
\begin{array}{@{}lll|l|l@{}}
F_1 & F_2 & F_3 & C & W \\
\hline
0 & 0 & 0 & T & 1 \\
1 & 0 & 0 & F & \alt<5->{\alert<5>{5}}{1} \\
1 & 1 & 0 & F & 1 \\
1 & 0 & 1 & T & \alt<9->{\alert<9>{4}}{1} \\
1 & 0 & 1 & T & \alt<9->{\alert<9>{4}}{1} \\
1 & 1 & 1 & F & 1 \\
\end{array}
$
\end{column}
\begin{column}{2.3in}
\uncover<2->{$
\begin{array}{@{}lll@{}}
F_1 \rightarrow \uncover<3->{\frac{4}{6}}
&
F_2 \rightarrow \uncover<3->{\frac{5}{6}}
&
F_3 \rightarrow \uncover<3->{\frac{4}{6}}
\end{array}
$} \\
\uncover<4->{Select $F_2$ model, update weights} \\
\medskip
\uncover<6->{$
\begin{array}{@{}lll@{}}
F_1 \rightarrow \uncover<7->{\frac{8}{10}}
&
F_2 \rightarrow \uncover<7->{\frac{5}{10}}
&
F_3 \rightarrow \uncover<7->{\frac{8}{10}}
\end{array}
$} \\
\uncover<8->{Select $F_1$ model, update weights} \\
\medskip
\uncover<10->{$
\begin{array}{@{}lll@{}}
F_1 \rightarrow \uncover<11->{\frac{8}{16}}
&
F_2 \rightarrow \uncover<11->{\frac{11}{16}}
&
F_3 \rightarrow \uncover<11->{\frac{14}{16}}
\end{array}
$} \\
\uncover<12->{Select $F_3$ model} \\
\end{column}
\end{columns}
\medskip
\uncover<13->{Classification by voting:}
\uncover<14->{$\frac{1}{4} \cdot \textit{model}_{F_1} + \frac{1}{5} \cdot \textit{model}_{F_2} + \frac{1}{7} \cdot \textit{model}_{F_3}$}
\\
\uncover<15->{Classifying the example $100$:}
\uncover<16->{
$
\frac{1}{4} \cdot \alt<17->{F}{?} +
\frac{1}{5} \cdot \alt<17->{T}{?} +
\frac{1}{7} \cdot \alt<17->{F}{?}
$}\uncover<18->{, so $F$}
\end{frame}

\begin{frame}{Boosting Properties}
\begin{block}{Complexity}
\begin{itemize}
\item Training time: \uncover<2->{$m$ times original}
\item Model size: \uncover<2->{$m$ times original}
\item Classification time: \uncover<2->{$m$ times original}
\end{itemize}
\uncover<3->{Usually acceptable when using e.g. decision stumps}
\end{block}
\begin{block}<4->{Theoretical Properties}
\begin{itemize}
\item With a large enough $m$, produces a perfect classifier
\item<5-> In practice, $m$ is selected through experimentation
\end{itemize}
\end{block}
\end{frame}

\subsection{Random Forests}

\begin{frame}[<+->]{Random Forests}
\begin{block}{Key Ideas}
\begin{itemize}
\item Voting works better when classifiers are independent
\item Partially random trees should be more independent
\end{itemize}
\end{block}
\begin{block}{Random Forests}
\begin{enumerate}
\item\label{sample-step} Generate a bootstrap sample of $n$ examples
\item\label{select-step} For each node, select $k$ features at random
\item\label{split-step} Split on the feature with the highest information gain
\item Repeat steps \ref{sample-step}, \ref{select-step} and \ref{split-step} to generate $n$ decision trees
\item Classify by taking the majority vote
\end{enumerate}
\end{block}
\end{frame}

\begin{frame}{Random Forest Exercise}
\begin{columns}
\begin{column}{1.2in}
\small
$
\begin{array}{lll|l}
F_0 & F_1 & F_2 & C \\
\hline
0   & 0   & 0   & F \\
0   & 0   & 1   & F \\
0   & 1   & 1   & F \\
1   & 0   & 1   & F \\
1   & 0   & 0   & T \\
1   & 1   & 1   & T \\
\end{array}
$
\end{column}
\begin{column}{2.8in}
\begin{itemize}
\item Generate a bootstrap sample
\item Example indices $[0, 5, 0, 1, 5, 4]$
\item Generate a random decision tree
\item Feature indices $[1, 2]$ then $[2, 0]$
\end{itemize}
\end{column}
\end{columns}
\pause
\begin{columns}
\begin{column}{1.2in}
\small
$
\begin{array}{lll|l}
F_0 & F_1 & F_2 & C \\
\hline
0   & 0   & 0   & F \\
1   & 1   & 1   & T \\
0   & 0   & 0   & F \\
0   & 0   & 1   & F \\
1   & 1   & 1   & T \\
1   & 0   & 0   & T \\
\end{array}
$
\end{column}
\begin{column}{2.8in}
\includegraphics[width=2in]{random-tree}
\end{column}
\end{columns}
\end{frame}

\begin{frame}[<+->]{Random Forest Properties}
\begin{block}{Disadvantages}
\begin{itemize}
\item Hard to estimate complexity due to random factor
\item $k$ (\# of features) must be determined experimentally
\end{itemize}
\end{block}
\begin{block}{Advantages}
\begin{itemize}
\item State of the art performance on many datasets
\item Relatively simple to implement
\item Expected error can be determined while training
\begin{itemize}
\item Bootstrap sample leaves out about $\frac{1}{3}$ of examples
\item Use these out-of-bag examples to test individual trees
\item Collect votes from all trees to get overall accuracy
\end{itemize}
\end{itemize}
\end{block}
\end{frame}

\part{Key Ideas}

\begin{frame}{Key Ideas}
\begin{block}{Supervised Learning}
\begin{itemize}
\item Input: $(x, f(x))$ examples
\item Output: $h$, a guess at $f$
\item Domain objects usually decomposed into features
\end{itemize}
\end{block}
\begin{block}{Decision Trees}
\begin{itemize}
\item Each node splits examples by feature values
\item Best feature yields the highest information gain
\item Bagging, boosting and random forests combine trees
\end{itemize}
\end{block}
\end{frame}

\end{document}


