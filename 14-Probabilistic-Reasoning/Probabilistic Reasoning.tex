% !TEX TS-program = pdflatexmk

\documentclass[14pt]{beamer}
\input{../slides-preamble}
\usepackage{hhline}

\title{Probabilistic Reasoning}
\date{13 Mar 2014}

\newcommand{\alertit}[1]{\alert{\textit{#1}}}

\tikzset{
  bn variable/.style={draw,ellipse,inner sep=0.2em},
  bn depends on/.style={draw,triangle 45-,thick},
  bn table/.style={draw,inner sep=0em,font=\small},
}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Outline}
\tableofcontents
\end{frame}

\section{Bayesian Networks}

\subsection{Bayesian Network Basics}

\begin{frame}[label=bayes-net-definition]{Bayesian Networks}
\begin{block}{Definition}
A \alert{Bayesian Network} is a data structure for representing independence relations among random variables
\end{block}
\begin{center}
\begin{tikzpicture}
\node[bn variable] at (1.5, 1.5) (A) {Age};
\node[bn variable] at (0, 0) (GH) {GrayHair};
\node[bn variable] at (3, 0) (B) {Bifocals};
\node[bn variable] at (6, 1.5) (BW) {BlazersWon};
\path[bn depends on] (GH) -- (A);
\path[bn depends on] (B) -- (A);
\end{tikzpicture}
\end{center}
\pause
$
\begin{array}{ll}
\lefteqn{P(\textit{Age}, \textit{GrayHair}, \textit{Bifocals}, \textit{BlazersWon}) = \mbox{}}\\ 
& P(\textit{Age}, \textit{GrayHair}, \textit{Bifocals})P(\textit{BlazersWon})
\end{array}
$
\pause
\smallskip
$
\begin{array}{ll}
\lefteqn{P(\textit{GrayHair},\textit{Bifocals}|\textit{Age}) = \mbox{}}\\ 
& P(\textit{GrayHair}|\textit{Age})P(\textit{Bifocals}|\textit{Age})
\end{array}
$ \\
\end{frame}

\begin{frame}{Bayesian Networks}
	\begin{block}{Components}
		\begin{itemize}
			\item Random variables (nodes)
			\item Directed links from \textit{parent} nodes to \textit{child} nodes
			\item $\mathbf{P}(X_{i}|\textit{Parents}(X_{i}))$ tables for each node
			\item Links form no cycles
		\end{itemize}
	\end{block}
	\pause
	\begin{block}{Intuitions}
		\begin{itemize}
			\item Links indicate \emph{direct} influence
			\item Causes usually near top
			\item Effects usually near bottom
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}[label=bayes-net-full-example]{Full Bayesian Network Example}
\begin{center}
\begin{tikzpicture}
\node[bn variable] at (0, 6) (B) {Burglary};
\node[bn table,right=0.2em of B] {
\begin{tabular}{ c }
\textbf{P(B)} \\
\hline
.001
\end{tabular}
};
\node[bn variable] at (6, 6) (E) {Earthquake};
\node[bn table,right=0.2em of E] {
\begin{tabular}{l}
\textbf{P(E)} \\
\hline
.002
\end{tabular}
};
\node[bn variable] at (3, 3) (A) {Alarm};
\node[bn table,right=1em of A] {
\begin{tabular}{ c c l }
\textbf{B} & \textbf{E} & \textbf{P(A|B,E)} \\
\hline
T & T & .95 \\
T & F & .94 \\
F & T & .29 \\
F & F & .001
\end{tabular}
};
\node[bn variable] at (0, 0) (J) {JohnCalls};
\node[bn table,right=0.2em of J] {
\begin{tabular}{ c l }
\textbf{A} & \textbf{P(J|A)} \\
\hline
T & .90 \\
F & .05
\end{tabular}
};
\node[bn variable] at (6, 0) (M) {MaryCalls};
\node[bn table,right=0.2em of M] {
\begin{tabular}{ c l }
\textbf{A} & \textbf{P(M|A)} \\
\hline
T & .70 \\
F & .01
\end{tabular}
};
\path[bn depends on] (A) -- (B);
\path[bn depends on] (A) -- (E);
\path[bn depends on] (J) -- (A);
\path[bn depends on] (M) -- (A);
\end{tikzpicture}
\end{center}
\end{frame}

\subsection{The Full Joint Distribution}
\begin{frame}{Representing the Full Joint Distribution}
	\begin{block}{Key Formula}
		$P(x_{1},\ldots,x_{n}) = \prod\limits_{i=1}^{n}{P(x_{i}|\textit{parents}(X_{i}))}$
	\end{block}
	\begin{block}{Example}
		$
		\begin{array}{lll}
			\lefteqn{P(j, m, a, \lnot b, \lnot e)}
			\\
			& = & \pause
			      P(j|\textit{parents}(j)) \cdot
			      P(m|\textit{parents}(m)) \cdot
			      \ldots
			\\
			& = & \pause
			      P(j|a) \cdot
			      P(m|a) \cdot
			      P(a|\lnot b, \lnot e) \cdot
			      P(\lnot b) \cdot
			      P(\lnot e)
			\\
			& = & \pause
			      0.90 \cdot
			      0.70 \cdot
			      0.001 \cdot
			      0.999 \cdot
			      0.998
			\\
			& = & \pause
			      0.00062
		\end{array}
		$
	\end{block}
\end{frame}
\begin{frame}{A Simple Inference Algorithm}
	\begin{block}{Goal: Answer Queries}
		\begin{itemize}
			\item One query variable given some evidence
			\item $\mathbf{P}(X|y_{1},\ldots,y_{n})$
		\end{itemize}
	\end{block}
	\pause
	\begin{block}{Solution: Enumeration}
		$
		\begin{array}{@{}lll@{}}
			\lefteqn{\mathbf{P}(X|y_{1},\ldots,y_{n})} \\
			& = & \alpha\mathbf{P}(X,y_{1},\ldots,y_{n}) \\
			& = & \alpha\sum\limits_{z_{1},\ldots,z_{k} \in \mathbf{\overline{XY}}}
			      {\mathbf{P}(X,y_{1},\ldots,y_{n},z_{1},\ldots,z_{k})} \\
			& = & \alpha\sum\limits_{z_{1},\ldots,z_{k} \in \mathbf{\overline{XY}}}
			      {\mathbf{P}(X|\ldots)
			       P(y_{1}|\ldots)
			       \ldots
			       P(z_{k}|\ldots)} \\
		\end{array}
		$
	\end{block}
\end{frame}

\begin{frame}[label=enumeration-worst-case]{Enumeration Worst Case}
\begin{block}{Enumeration Formula}
$
\mathbf{P}(X|y_{1},\ldots,y_{n}) = 
\alpha\sum\limits_{z_{1},\ldots,z_{k} \in \mathbf{\overline{XY}}}
          {\mathbf{P}(X|\mbox{\scriptsize\ldots})
           P(y_{1}|\mbox{\scriptsize\ldots})
           \ldots
           P(z_{k}|\mbox{\scriptsize\ldots})}
$
\end{block}
\pause
\begin{columns}
\begin{column}{1.5in}
\begin{center}
\begin{tikzpicture}
\node[bn variable] at (1, 3.6) (1) {$X_1$};
\node[bn variable] at (2, 2.7) (2) {$X_2$}
  edge[bn depends on] (1);
\node[bn variable] at (0, 1.8) (3) {$X_3$}
  edge[bn depends on] (1)
  edge[bn depends on] (2);
\node[bn variable] at (2, .9) (4) {$X_4$}
  edge[bn depends on] (1)
  edge[bn depends on] (2)
  edge[bn depends on] (3);
\node[bn variable] at (0, 0) (5) {$X_5$}
  edge[bn depends on] (1)
  edge[bn depends on] (2)
  edge[bn depends on] (3)
  edge[bn depends on] (4);
\end{tikzpicture}
\smallskip
Query: $P(X_{5})$
\end{center}
\end{column}
\pause
\begin{column}{2.5in}
\begin{block}{Worst Case in General}
\begin{itemize}
\item $\approx n$ parents per node
\item $\approx n$ variables not in query
\item $d$ values per variable
\end{itemize}
Time Complexity: \pause $O(nd^{n})$
\end{block}
\end{column}
\end{columns}
\end{frame}

\subsection{Constructing Bayesian Networks}
\begin{frame}{Avoiding Fully Connected Networks}
	\begin{block}{Principles}
		\begin{enumerate}
			\item Add root causes
			\item\label{add-leaf-effects} Add variables directly influenced by leaves
			\item If variables left, goto \ref{add-leaf-effects}
		\end{enumerate}
	\end{block}
	\medskip
	\begin{columns}
		\begin{column}{2.2in}
			\uncover<2->{
			Good:
			\\
			\footnotesize \textit{Age}, \textit{GrayHair}, \textit{Bifocals}, \textit{ReadDist}} \\
			\centering
			\only<-3>{\invisible<-2>{\includegraphics[height=1.2in]{age_hair_vision_1}}}%
			\only<4>{\includegraphics[height=1.2in]{age_hair_vision_2}}%
			\only<5>{\includegraphics[height=1.2in]{age_hair_vision_3}}%
			\only<6->{\includegraphics[height=1.2in]{age_hair_vision_4}}%
		\end{column}
		\begin{column}{2.2in}
			\uncover<7->{
			Bad:
			\\
			\footnotesize \textit{ReadDist}, \textit{GrayHair}, \textit{Bifocals}, \textit{Age}} \\
			\centering
			\only<-8>{\invisible<-7>{\includegraphics[height=1.2in]{vision_hair_age_1}}}%
			\only<9>{\includegraphics[height=1.2in]{vision_hair_age_2}}%
			\only<10>{\includegraphics[height=1.2in]{vision_hair_age_3}}%
			\only<11->{\includegraphics[height=1.2in]{vision_hair_age_4}}%
		\end{column}
	\end{columns}
\end{frame}
\begin{frame}{Bayesian Network Exercise}
	\begin{block}{Construct a Network}
		\begin{itemize}
			\item The fire alarm usually goes off when there's a fire
			\item When the alarm rings everyone usually exits together
			\item Most of the time there's smoke when there's a fire
			\item Someone sometimes pulls the fire alarm ``as a joke''
			\item The fire trucks usually come when the alarm goes off
			\item Sometimes everyone exits together for a picnic
		\end{itemize}
	\end{block}
\end{frame}
\begin{frame}{Bayesian Network Exercise}
	One possible solution:
	\begin{center}
		\includegraphics[height=2in]{fire_alarm}
	\end{center}
\end{frame}


\section{Efficient Exact Inference}
\subsection{Moving Summation Terms}
\begin{frame}{Drawbacks of Simple Enumeration}
	\begin{block}{Recall: Simple Enumeration}
		Time Complexity: $O(nd^{n})$
	\end{block}
	\pause
	\begin{block}{Example}
		$
		\begin{array}{@{}lll@{}}
		P(b|j,m) & = & \alpha\sum\limits_{e}\sum\limits_{a}P(b)P(e)P(a|b,e)P(j|a)P(m|a) \\
		         & = & \alpha(P(b)P(e)P(a|b,e)P(j|a)P(m|a) + \mbox{}\\
		         &   &   \tab P(b)P(e)P(\lnot a|b,e)P(j|\lnot a)P(m|\lnot a) + \mbox{}\\
		         &   &   \tab P(b)P(\lnot e)P(a|b,\lnot e)P(j|a)P(m|a) + \mbox{}\\
		         &   &   \tab P(b)P(\lnot e)P(\lnot a|b,\lnot e)P(j|\lnot a)P(m|\lnot a))
		\end{array}
		$
		\\
		\medskip
		\pause Problem: \pause We calculate $P(b)$, $P(e)$ and $P(\lnot e)$ many times!
	\end{block}
\end{frame}
\begin{frame}{More Intelligent Summation}
	\begin{block}{Moving Terms in Algebra}
		$
		\begin{array}{lll}
		\lefteqn{abd + abe + acf + acg} \\
		   & = & \pause a(bd + be + cf + cg) \\
		   & = & \pause a(b(d + e) + c(f + g)) \\
		\end{array}
		$
	\end{block}
	\pause
	\begin{block}{Moving Terms in Bayesian Network Calculations}
		$
		\begin{array}{lll}
		P(b|j,m) & = & \alpha\sum\limits_{e}\sum\limits_{a}P(b)P(e)P(a|b,e)P(j|a)P(m|a) \\
		         & = & \pause
		               \alpha P(b)\sum\limits_{e}\sum\limits_{a}P(e)P(a|b,e)P(j|a)P(m|a) \\
		         & = & \pause
		               \alpha P(b)\sum\limits_{e}P(e)\sum\limits_{a}P(a|b,e)P(j|a)P(m|a)
		\end{array}
		$
	\end{block}
\end{frame}

\subsection{Dynamic Programming}
\begin{frame}{Calculations through Depth First Search}
	Example formula: \\
	\tab $P(b|j,m) = \alpha P(b)\sum\limits_{e}P(e)\sum\limits_{a}P(a|b,e)P(j|a)P(m|a)$ \\
	\pause
	\bigskip
	Example tree: \\
	\vspace{-2em}
	\begin{center}
		\includegraphics[height=2in]{summation_tree}
	\end{center}
\end{frame}
\begin{frame}{Eliminating More Redundancies}
	\begin{columns}
		\begin{column}{2.2in}
			\begin{block}{Problem}
				\begin{itemize}
					\item Moving terms got rid of some redundancies
					\item But notice: \\
					      $P(j|a)P(m|a)$ \\
					      $P(j|\lnot a)P(m|\lnot a)$
				\end{itemize}
			\end{block}
			\begin{block}<2->{Solution}
				\begin{itemize}
					\item<3-> Dynamic programming!
					\item<4-> Store partial calculations
					\item<4-> Look up repeated terms
				\end{itemize}
			\end{block}
		\end{column}
		\begin{column}{2.4in}
			\includegraphics[width=2.4in]{summation_tree_narrow}
		\end{column}
	\end{columns}
\end{frame}

\subsection{Exact Inference Properties}
\begin{frame}{Exact Inference Properties}
	\begin{block}{Moving Summation Terms}
		Given $n$ boolean variables:
		\\ \medskip
		\begin{tabular}{ll}
			\keyword{Time Complexity?}  & \pause $O(2^{n})$, explores whole tree
			\\ \pause
			\keyword{Space Complexity?} & \pause $O(n)$, depth first search
		\end{tabular}
	\end{block}
	\pause
	\begin{block}{With Dynamic Programming}
		\begin{tabular}{@{}ll@{}}
			\lefteqn{\mbox{\keyword{Singly connnected networks}}} \\
			& Time and space $O(n)$ if number of parents is bounded \\
			\pause
			\lefteqn{\mbox{\keyword{Multiply connected networks}}} \\
			& Time and space can be as bad as $O(k^{n})$
		\end{tabular}
	\end{block}
\end{frame}


\section{Approximate Inference}
\begin{frame}{Approximate Inference}
	\begin{block}{Motivation}
		\begin{itemize}
			\item Exact inference is exponential in number of variables
			\item Sometimes an approximate result is enough
				\begin{itemize}
					\item E.g. 29.6\% vs. 30.0\% chance of rain
				\end{itemize}
		\end{itemize}
	\end{block}
	\pause
	\begin{block}{General Approach for Approximate Inference}
		\begin{itemize}
			\item Generate a bunch of full variable assignments
			\item Make sure assignments are consistent with evidence
			\item Count times $X\!=\!x$ and divide by number of samples
		\end{itemize}
	\end{block}
\end{frame}
\begin{frame}[fragile]{Generating Full Variable Assignments}
	\begin{block}{Key Ideas for Creating a Random Assignment}
		\begin{itemize}
			\item Nodes are ordered from parents to children
			\item Single nodes calculate $P(x_{i}|\textit{parents}(X_{i}))$
		\end{itemize}
	\end{block}
	\pause
	\bigskip
	\begin{semiverbatim}\scriptsize\bfseries
		\keyword{def} \defn{prior_sample}(bayes_net):
		    \pause\cmnt{# generate a value for each variable from roots to leaves}
		    sample = []
		    \keyword{for} node \keyword{in} bayes_net:
		        \pause\cmnt{# find the values assigned to the parents}
		        parent_samples = [sample[p.index] \keyword{for} p \keyword{in} node.parents]
		        \pause\cmnt{# find the probability for this assignment from the table}
		        prob = node.get_prob(\keyword{True}, *parent_samples)
		        \pause\cmnt{# add True or False according to the distribution}
		        sample.append(random.random() < prob)
		    \pause\cmnt{# return the complete sample}
		    \keyword{return} sample
	\end{semiverbatim}
\end{frame}


\subsection{Rejection Sampling}
\begin{frame}[fragile]{Rejection Sampling}
	\begin{block}{Key Idea}
		Throw away samples inconsistent with the evidence
	\end{block}
	\medskip
	\pause
	\begin{tabular}{ll}
		Variables: & \textit{Cloudy}, \textit{Sprinkler}, \textit{Rain}, \textit{WetGrass} \\
		Query:     & $P(\textit{Rain}|\textit{Sprinkler}\!=\!\textit{true})$
	\end{tabular}
	\\
	\medskip
	$
	\begin{array}{llll|cc}
		\textit{Cloudy} & \textit{Sprinkler} & \textit{Rain}  & \textit{WetGrass} & \textit{rain} & \lnot\textit{rain} \\
		\hline
		\pause
		\textit{false}  & \textit{false}     & \textit{false} & \textit{false}    &           &            \\
		\pause
		\textit{true}   & \textit{true}      & \textit{false} & \textit{true}     &           & \pause 1   \\
		\pause
		\textit{true}   & \textit{false}     & \textit{false} & \textit{false}    &           &            \\
		\pause
		\textit{false}  & \textit{true}      & \textit{true}  & \textit{true}     & \pause 1  &            \\
		\pause
		\textit{false}  & \textit{true}      & \textit{false} & \textit{true}     &           & \pause 1
	\end{array}
	$
	\\
	\medskip
	\pause
	\tab$P(\textit{Rain}\!=\!\textit{true}|\textit{Sprinkler}\!=\!\textit{true})=\pause\frac{1}{1 + 1 + 1}=\frac{1}{3}$
\end{frame}
\begin{frame}[fragile]{Rejection Sampling Code}
	\begin{semiverbatim}\scriptsize\bfseries
		\keyword{def} \defn{rejection_sampling}(query_var, evidence, bayes_net, samples):
		    \pause\cmnt{# generate a bunch of samples, counting query values}
		    counts = \{\keyword{False}: \num{0}, \keyword{True}: \num{0}\}
		    \keyword{for} _ \keyword{in} range(samples):
		        sample = prior_sample(bayes_net)
		        \pause\cmnt{# if the sample is consistent with the evidence, count it}
		        \keyword{if} all(sample[v.index] == evidence[v] \keyword{for} v \keyword{in} evidence):
		            counts[sample[query_var.index]] += \num{1}
		    \pause\cmnt{# normalize the counts and return the probabilities}
		    \keyword{return} normalize(counts)
		\pause
		\keyword{def} \defn{normalize}(counts):
		    \pause\cmnt{# divide all counts by the total}
		    total = sum(counts.values())
		    \keyword{for} value \keyword{in} counts:
		        counts[value] /= total
		    \pause\cmnt{# return the probabilities}
		    \keyword{return} counts
	\end{semiverbatim}
\end{frame}
\begin{frame}{Rejection Sampling Exercise}
	\begin{columns}
		\begin{column}{2.7in}
			\includegraphics[width=2.7in]{rain_net}
		\end{column}
		\begin{column}{1.5in}
			\small
			Calculate: \\
			\smallskip
			\begin{tabular}{l@{}}
				$P(\textit{rain}|\textit{sprinkler})$ \\
				5 samples \\
				\textit{true} if below random
			\end{tabular} \\
			\medskip
			Random numbers: \\
			\smallskip
			$
			\begin{array}{llll}
				\parbox{.22in}{C} & \parbox{.22in}{S}            & \parbox{.22in}{R}             & WG                \\
				\hline
				\alt<2->{F}{0.6}  & \alert<8->{\alt<3->{T}{0.4}} & \alt<4->{F}{0.3}              & \alt<5->{T}{0.8} \\
				\alt<6->{F}{0.7}  & \alert<8->{\alt<6->{T}{0.3}} & \alt<6->{F}{0.8}              & \alt<6->{T}{0.6} \\
				\alt<6->{T}{0.3}  &            \alt<6->{F}{0.2}  & \alt<6->{T}{0.7}              & \alt<6->{T}{0.3}  \\
				\alt<6->{F}{0.9}  & \alert<8->{\alt<6->{T}{0.2}} & \alt<6->{F}{0.4}              & \alt<6->{T}{0.1} \\
				\alt<6->{F}{0.8}  & \alert<8->{\alt<6->{T}{0.4}} & \alert<9->{\alt<6->{T}{0.1}}  & \alt<6->{T}{0.9} \\
			\end{array}
			$ \\
			\medskip
			\uncover<7->{$P(\textit{rain}|\textit{sprinkler}) = \uncover<10->{\frac{1}{4}}$}
		\end{column}
	\end{columns}
\end{frame}
\begin{frame}{Rejection Sampling}
	\begin{block}{Properties}
		Given $n$ variables, at most $d$ parents each, $s$ samples drawn and $u$ samples used:
		\begin{itemize}
			\item Time Complexity: \pause $O(nds)$
			\pause
			\item Standard deviation of error proportional to $\frac{1}{\sqrt{u}}$ \\
			      \pause
			      i.e. it approximates the true probability
		\end{itemize}
	\end{block}
	\pause
	\begin{block}{Problems}
		\begin{itemize}
			\item Generates and throws away many samples
			\item More thrown away for lower probability evidence
			\item More evidence variables means lower probability
		\end{itemize}
	\end{block}
\end{frame}
\begin{frame}{So Why Not Just Fix The Evidence?}
	\begin{tabular}{@{}l@{\hspace{1.5em}}l@{}}
		Given:
		$
		\begin{array}[t]{llll}
			P(A) & & A & P(B) \\
			\hhline{-~--}
			0.4  & & T & 0.2 \\
			     & & F & 0.4 \\
		\end{array}
		$
		&
		Query: $P(A|b)$
		\\
		\\
		\multicolumn{2}{@{}l@{}}{\pause Using the full joint distribution:}
		\\
		$
		\begin{array}[t]{ll|l}
			A & B & P(A,B) \\
			\hline
			\pause T & T & \pause 0.4 \cdot 0.2 = 0.08 \\
			\pause T & F & \pause 0.4 \cdot 0.8 = 0.32 \\
			\pause F & T & \pause 0.6 \cdot 0.4 = 0.24 \\
			\pause F & F & \pause 0.6 \cdot 0.6 = 0.36 \\
		\end{array}
		$
		&
		$
		\pause
		\begin{array}[t]{lll}
			P(A|b) & = & \pause \alpha P(A, b) \\
			       & = & \pause \alpha \langle 0.08, 0.24 \rangle \\
			       & = & \pause \langle 0.25, 0.75 \rangle
		\end{array}
		$
		\\
		\\
		\multicolumn{2}{@{}l@{}}{\pause Fixing $B\!=\!b$, and since $P(A) = 0.4$ we generate one of:}
		\\
		$
		\begin{array}[t]{ll}
			\pause\langle a, b \rangle       & \pause\mbox{40\% of the time} \\
			\pause\langle \lnot a, b \rangle & \pause\mbox{60\% of the time} \\
		\end{array}
		$
		&
		$
		\pause
		\begin{array}[t]{l}
			P(A|b) = \pause\langle 0.4, 0.6 \rangle \\
			\pause\mbox{\alert{But that's wrong!}}
		\end{array}
		$
	\end{tabular}
\end{frame}

\subsection{Likelihood Weighting}
\begin{frame}{Likelihood Weighting}
	\begin{block}{Key Ideas}
		\begin{itemize}
			\item Only generate samples consistent with evidence
			\item Use $P(X\!=\!x|\textit{parents}(X))$ to assign weights
		\end{itemize}
	\end{block}
	\medskip
	\pause
	\begin{tabular}{ll}
		Variables: & \textit{Cloudy}, \textit{Sprinkler}, \textit{Rain}, \textit{WetGrass} \\
		Query:     & $P(\textit{Rain}|\textit{Sprinkler}\!=\!\textit{true})$
	\end{tabular}
	\\
	\smallskip
	$
	\begin{array}{llll|cc}
		\textit{Cloudy} & \textit{Sprinkler} & \textit{Rain}  & \textit{WetGrass} & \textit{rain}  & \lnot\textit{rain} \\
		\hline
		\pause
		\textit{true}   & \textit{true}      & \textit{true}  & \textit{true}     & \pause 0.1 &                \\
		\pause
		\textit{true}   & \textit{true}      & \textit{true}  & \textit{true}     & \pause 0.1 &                \\
		\pause
		\textit{false}  & \textit{true}      & \textit{false} & \textit{true}     &            & \pause 0.5     \\
		\pause
		\textit{true}   & \textit{true}      & \textit{true}  & \textit{true}     & \pause 0.1 &                \\
	\end{array}
	$
	\\
	\smallskip
	\pause
	\tab$P(\textit{Rain}\!=\!\textit{true}|\textit{Sprinkler}\!=\!\textit{true})=\pause\frac{0.1 + 0.1 + 0.1}{0.1 + 0.1 + 0.1 + 0.5} = \frac{3}{8}$
\end{frame}
\begin{frame}[fragile]{Likelihood Weighting Code}
	\begin{semiverbatim}\scriptsize\bfseries
		\keyword{def} \defn{likelihood_weighting}(query_var, evidence, bayes_net, samples):
		    \pause\cmnt{# generate samples, adding up weights for each query value}
		    counts = \{\keyword{False}: \num{0}, \keyword{True}: \num{0}\}
		    \keyword{for} _ \keyword{in} range(samples):
		        sample, weight = weighted_sample(bayes_net, evidence)
		        counts[sample[query_var.index]] += weight
		    \pause\keyword{return} normalize(counts)
		\pause
		\keyword{def} \defn{weighted_sample}(bayes_net, evidence):\pause
		    sample, weight = [], \num{1.0}
		    \keyword{for} node \keyword{in} bayes_net:
		        p_samples = [sample[p.index] \keyword{for} p \keyword{in} node.parents]
		        \pause\cmnt{# if the value is given, add it and update the weight}
		        \keyword{if} node \keyword{in} evidence:
		            weight *= node.get_prob(evidence[node], *p_samples)
		            sample.append(evidence[node])
		        \pause\cmnt{# otherwise, add True or False using the distribution}
		        \keyword{else}:
		            prob = node.get_prob(\keyword{True}, *p_samples)
		            sample.append(random.random() < prob)
		    \pause\keyword{return} sample, weight
	\end{semiverbatim}
\end{frame}
\begin{frame}{Likelihood Weighting}
	\begin{block}{Properties}
		Given $n$ variables, $\leq d$ parents each, and $s$ samples:
		\begin{itemize}
			\item Time Complexity: \pause $O(nds)$
			\pause
			\item Unlike rejection sampling, all samples are used
		\end{itemize}
	\end{block}
	\pause
	\begin{block}{Problems}
		\begin{itemize}
			\item More evidence variables \\
			      $\rightarrow$ each sample has lower probability
			\pause
			\item Evidence late in the node ordering \\
			      $\rightarrow$ ealier node selections may not match evidence
		\end{itemize}
	\end{block}
\end{frame}

\subsection{Gibbs Sampling}
\begin{frame}{Gibbs Sampling}
	\begin{block}{Gibbs Sampling (a Markov chain Monte Carlo algorithm)}
		\begin{itemize}
			\item Start by randomly assigning values to variables
			\item Iteratively update values given current assignment
				\begin{itemize}
					\item Assign new values given ``surrounding'' distribution
				\end{itemize}
		\end{itemize}
	\end{block}
	\pause
	\begin{block}{Gibbs Sampling for Bayesian Networks}
		Define ``surrounding'' as the \alert{Markov Blanket}: \\
		\tab\tab a node's parents, children and children's parents
	\end{block}
\end{frame}
\begin{frame}{Gibbs Sampling Example}
	\begin{tabular}{ll}
		Variables: & \textit{Cloudy}, \textit{Sprinkler}, \textit{Rain}, \textit{WetGrass} \\
		Query:     & $P(\textit{Rain}|\textit{Sprinkler}\!=\!\textit{true})$
	\end{tabular}
	\\
	\bigskip
	$
	\begin{array}{llll|cc}
		\textit{Cloudy}           & \textit{Sprinkler} & \textit{Rain }  & \textit{WetGrass} & \textit{rain}  & \lnot\textit{rain} \\
		\hline
		\only<2>{\textit{true}    & \textit{true}      & \textit{true}   & \textit{false}    & 0          & 0}
		\only<3>{\alertit{false}  & \textit{true}      & \textit{true}   & \textit{false}    & 1          & 0}
		\only<4>{\textit{false}   & \textit{true}      & \alertit{false} & \textit{false}    & 1          & 1}
		\only<5>{\textit{false}   & \textit{true}      & \textit{false}  & \alertit{true}    & 1          & 2}
		\only<6>{\alertit{false}  & \textit{true}      & \textit{false}  & \textit{true}     & 1          & 3}
		\only<7>{\textit{false}   & \textit{true}      & \alertit{true}  & \textit{true}     & 2          & 3}
		\only<8>{\textit{false}   & \textit{true}      & \textit{true}   & \alertit{true}    & 3          & 3}
		\only<9>{\alertit{true}   & \textit{true}      & \textit{true}   & \textit{true}     & 4          & 3}
		\only<10>{\textit{true}   & \textit{true}      & \alertit{false} & \textit{true}     & 4          & 4}
		\only<11>{\textit{true}   & \textit{true}      & \textit{false}  & \alertit{true}    & 4          & 5}
		\only<12>{\alertit{false} & \textit{true}      & \textit{false}  & \textit{true}     & 4          & 6}
		\only<13>{\textit{false}  & \textit{true}      & \alertit{false} & \textit{true}     & 4          & 7}
		\only<14->{\textit{false}  & \textit{true}      & \textit{false}  & \alertit{true}    & 4          & 8}
		\\
	\end{array}
	$
	\\
	\medskip
	\tab\uncover<15->{$P(\textit{Rain}\!=\!\textit{true}|\textit{Sprinkler}\!=\!\textit{true}) = \displaystyle
	                   \uncover<16->{\frac{4}{4 + 8} = \uncover<17->{\frac{1}{3}$}}}
	\\
	\medskip
	\begin{block}<18>{Why Gibbs Sampling Works}
		\begin{itemize}
			\item Over time, reaches ``dynamic equilibrium''
			\item Time spent in each state proportional to its probability
		\end{itemize}
	\end{block}
\end{frame}
\begin{frame}[fragile]{Gibbs Sampling Code}
	\vspace{-.5em}
	\begin{semiverbatim}\scriptsize\bfseries
		\keyword{def} \defn{gibbs_sampling}(query_var, evidence, bayes_net, samples):
		    \pause\cmnt{# initialize the sample with random values for non-evidence}
		    sample = []
		    \keyword{for} node \keyword{in} bayes_net:
		        \keyword{if} node \keyword{in} evidence:
		            sample.append(evidence[node])
		        \keyword{else}:
		            sample.append(random.random() < \num{0.5})
		    \pause\cmnt{# generate samples by changing non-evidence values}
		    counts = \{\keyword{False}:\num{0}, \keyword{True}:\num{0}\}
		    non_evidence = [n \keyword{for} n \keyword{in} bayes_net \keyword{if} n \keyword{not in} evidence]
		    \keyword{for} _ \keyword{in} range(samples):
		        \keyword{for} node \keyword{in} non_evidence:
		            \pause\cmnt{# get the prob distribution given the markov blanket}
		            probs = get_probs_given_markov_blanket(node, sample)
		            \pause\cmnt{# select a new value according to that distribution}
		            sample[node.index] = random.random() < probs[\keyword{True}]
		            \pause\cmnt{# increment the count for the current query value}
		            counts[sample[query_var.index]] += \num{1}
		    \pause\cmnt{# normalize the counts and return the probabilities}
		    \keyword{return} normalize(counts)
	\end{semiverbatim}
\end{frame}
\begin{frame}[fragile]{Markov Blanket Code}
	\begin{block}{Markov Blanket Probability}
	\small 
	$P(x|\textit{mb}(X)) = \alpha P(x|\textit{parents}(X)) \prod\limits_{Y \in \textit{\scriptsize Children}(X)}P(y|\textit{parents}(Y))$
	\vspace{-.25em}
	\end{block}
	\vspace{.25em}
	\pause
	\begin{semiverbatim}\scriptsize\bfseries
		\keyword{def} \defn{get_probs_given_markov_blanket}(node, sample):
		    \pause\cmnt{# get the probabilities for each value of the variable}
		    counts = \{\}
		    \keyword{for} value \keyword{in} [\keyword{True}, \keyword{False}]:
		        \pause\cmnt{# change the node's value in the sample}
		        sample[node.index] = value
		        \pause\cmnt{# the probability of the node given its parents}
		        p_samples = [sample[p.index] \keyword{for} p \keyword{in} node.parents]
		        counts[value] = node.get_prob(value, *p_samples)
		        \pause\cmnt{# times probabilities of children given their parents}
		        \keyword{for} child \keyword{in} node.children:
		            c_sample = sample[child.index]
		            p_samples = [sample[p.index] \keyword{for} p \keyword{in} child.parents]
		            counts[value] *= child.get_prob(c_sample, *p_samples)
		    \pause\keyword{return} normalize(counts)
	\end{semiverbatim}
\end{frame}
\begin{frame}{Gibbs Sampling Exercise}
	\begin{columns}
		\begin{column}{2.2in}
			\includegraphics[width=2.2in]{rain_net}
		\end{column}
		\begin{column}{2in}
			\small
			Initial Sample: $\langle \lnot c, s, \lnot r, w \rangle$ \\
			\smallskip
			\pause
			Updating $C$: \\
			\smallskip
			$
			\begin{array}{l@{}l}
			\pause
			\lefteqn{P(c|\textit{mb}(C))} \\
			\pause
			& = \alpha P(c)P(s|c)P(\lnot r|c) \\
			\pause
			& = \alpha \cdot 0.5 \cdot 0.1 \cdot 0.2 = 0.01\alpha \\
			\pause
			\lefteqn{P(\lnot c|\textit{mb}(C))} \\
			\pause
			& = \alpha P(\lnot c)P(s|\lnot c)P(\lnot r|\lnot c) \\
			\pause
			& = \alpha \cdot 0.5 \cdot 0.5 \cdot 0.8 = 0.2\alpha \\
			\pause
			\lefteqn{\mathbf{P}(C|\textit{mb}(C)) = \left\langle 0.048, 0.952 \right\rangle}
			\end{array}
			$ \\
			\smallskip
			\pause
			Random 0.03\pause: $\langle c, s, \lnot r, w \rangle$ \\
			\medskip
			\pause
			\begin{tabular}{@{}llll@{}}
				\bfseries Update & $R$  & $W$  & $C$ \\
				\bfseries Random & 0.48 & 0.63 & 0.83 \\
				\pause
				\bfseries Result & \lefteqn{\langle \lnot c, s, r, w \rangle}
			\end{tabular}
		\end{column}
	\end{columns}
\end{frame}
\begin{frame}{Gibbs Sampling}
	\begin{block}{Properties}
		Given $n$ variables, $s$ samples, and $\leq d$ nodes $\in \textit{mb}(X)$:
		\begin{itemize}
			\item Time Complexity: \pause $O(nds)$
			\pause
			\item Unlike rejection sampling, all samples are used
			\pause
			\item Performs well in practice
		\end{itemize}
	\end{block}
	\pause
	\begin{block}{Problems}
		\begin{itemize}
			\item Difficult to tell when convergence is achieved
			\item Performs worse when Markov blankets are large
		\end{itemize}
	\end{block}
\end{frame}


\part{Key Ideas}
\begin{frame}{Key Ideas}
	\begin{block}{Bayesian Networks}
		\begin{itemize}
			\item Variables linked by conditional independence
			\item Put causes on top, add direct effects below
			\item $P(x_{1},\ldots,x_{n}) = \prod\limits_{i=1}^{n}{P(x_{i}|\textit{parents}(X_{i}))}$
		\end{itemize}
	\end{block}
	\begin{block}{Inference Methods}
		\begin{itemize}
			\item Exact: sum reordering and dynamic programming
			\item Rejection sampling requires many samples
			\item Likelihood weighting poor with a lot of evidence
			\item Gibbs Sampling updates based on Markov blanket
		\end{itemize}
	\end{block}
\end{frame}

\end{document}


