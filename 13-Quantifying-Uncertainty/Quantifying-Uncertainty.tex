% !TEX TS-program = pdflatexmk

\documentclass[14pt]{beamer}
\input{../slides-preamble}
\usepackage{hhline}

\title{Quantifying Uncertainty}
\date{4 Mar 2014}


\newcommand{\tinywumpusworldexample}[3]{
{\tiny
\begin{wumpusgrid}{3}{0.055\textwidth}
\wumpusbottom{1-}{0}{0}{$\lnot P$}
\wumpusbottom{1-}{0}{1}{$\lnot P$}
\wumpusbottom{1-}{1}{0}{$\lnot P$}
\wumpuspercept{1-}{0}{1}{$\scriptstyle B$}
\wumpuspercept{1-}{1}{0}{$\scriptstyle B$}
\wumpusbottom{1-}{0}{2}{$\mathbf{#1}$}
\wumpusbottom{1-}{1}{1}{$\mathbf{#2}$}
\wumpusbottom{1-}{2}{0}{$\mathbf{#3}$}
\end{wumpusgrid}}
}

\pgfmathdeclarefunction{gauss}{2}{%
  \pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((\x-#1)^2)/(2*#2^2))}%
}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Outline}
\tableofcontents
\end{frame}

\section{Probability Theory}
\begin{frame}{Motivation}
	\begin{block}{Goal}
		Look out window - is it warm?
	\end{block}
	\pause
	\begin{block}{Rules}
		$
		\begin{array}{l}
			\forall d\ \textit{Sunny}(d) \Rightarrow \textit{Warm}(d) \\
			\pause \mbox{Wrong; it can be sunny but cold} \\
			\pause \forall d\ \textit{Warm}(d) \Rightarrow \textit{Sunny}(d) \\
			\pause\mbox{Wrong; it can be warm and cloudy} \\
		\end{array}
		$
	\end{block}
	\pause
	\begin{block}{Problem}
		Can't capture that sunny days are usually warm
	\end{block}
\end{frame}


\subsection{Probability Basics}

\begin{frame}{Probability Theory}
\begin{block}{Key Idea}
Measure degrees of belief in propositions
\end{block}
\bigskip
\begin{description}[Random Variable]
\pause
\item[Random Variable]
An interesting part of the world \\
e.g. $\textit{Sky}$ or $\textit{Temp}$
\pause
\item[Domain]
Possible values of a random variable \\
e.g. $\textit{domain}(\textit{Temp})=\langle\textit{warm}, \textit{cold}, \ldots\rangle$
\pause
\item[Proposition]
A statement, like propositional logic \\
e.g. $\textit{Sky} = \textit{sunny} \land \textit{Temp} = \textit{warm}$
\pause
\item[Probability]
The degree of belief in a proposition \\
e.g. $P(\textit{Temp} = \textit{warm}) = 0.7$
\end{description}
\end{frame}

\begin{frame}{Aside: Fuzzy Logic}
	\begin{block}{Probability Theory}
		\begin{itemize}
			\item Propositions are believed to a certain degree
			\item Belief values range from [0, 1]
		\end{itemize}
	\end{block}
	\pause
	\begin{block}{Fuzzy Logic}
		\begin{itemize}
			\item Propositions are \textit{true} to a certain degree
			\item \textit{Truth} values range from [0, 1]
		\end{itemize}
		\pause
		For example:
		\begin{itemize}
			\item $T(\textit{Tall}(\textit{Steve})) = 0.5$
			\item $T(\textit{Fat}(\textit{Steve})) = 0.1$
		\end{itemize}
		\pause
		Better for describing indefinite classes than for reasoning
	\end{block}
\end{frame}

\begin{frame}[label=probability-rules]{Probability Rules}
\begin{block}{Range of Probabilities}
$0 \leq P(a) \leq 1$
\end{block}
\begin{block}{Propositions Known to be True or False}<2->
$
\begin{array}{@{} l l l @{} }
P(\textit{true})  & = & 1 \\
P(\textit{false}) & = & 0 \\
\end{array}
$
\end{block}
\begin{block}{Probability of Disjunctions}<3->
\begin{tabular}{@{} l @{} l @{} }
$P(a \lor b) = P(a) + P(b) - P(a \land b)$
&
\uncover<4->{
\small
\raisebox{-.5\height}{
\begin{tikzpicture}
\draw[red,fill=red,opacity=0.5] (0.4,0) circle [radius=1.25];
\draw[blue,fill=blue,opacity=0.5] (1.6,0) circle [radius=1.25];
\node at (-0.2,0) {$a$};
\node at (1,0) {$a \land b$};
\node at (2.2,0) {$b$};
\end{tikzpicture}}}
\end{tabular}
\end{block}
\end{frame}

\begin{frame}{Random Variables}
	\begin{block}{Boolean Random Variables}
		Have the domain $\langle \textit{true}, \textit{false} \rangle$, e.g.
		\begin{itemize}
			\item \textit{IsSunny}
		\end{itemize}
	\end{block}
	\pause
	\begin{block}{Discrete Random Variables}
		Have a countable domain, e.g.
		\begin{itemize}
			\item $\textit{domain}(\textit{Sky}) =
			       \langle \textit{sunny}, \textit{cloudy}, \ldots \rangle$
			\item $\textit{domain}(\textit{DieRoll}) =
			       \langle 1, 2, 3, 4, 5, 6 \rangle$
		\end{itemize}
	\end{block}
	\pause
	\begin{block}{Continuous Random Variables}
		Have a real-valued domain, e.g.
		\begin{itemize}
			\item $\textit{domain}(\textit{Temperature}) =
			       \langle \ldots, -40^{\circ}, \ldots, 98.6^{\circ}, \ldots \rangle$
		\end{itemize}
	\end{block}
\end{frame}


\subsection{Prior, Joint and Conditional Probabilities}
\begin{frame}{Prior Probability}
\setbeamercovered{invisible}
\begin{block}{Definition}
\begin{tabular}{lm{3.5in}}
\large $P(a)$
&
The \alert{unconditional} or \alert{prior probability} of a proposition $a$ is the degree of belief in that proposition \emph{given no other information}
\end{tabular}
\end{block}
\pause
\begin{block}{Examples}
$
\begin{array}{lll}
P(\textit{DieRoll} = 5) & = & \pause 1/6 \\
\pause
P(\textit{CardDrawn} = \mbox{A}\spadesuit) & = & \pause 1/52 \\
\pause
P(\textit{SkyInBirmingham} = \textit{sunny}) & = & \pause 210/365
\end{array}
$
\end{block}
\end{frame}

\begin{frame}{Joint Probability}
\setbeamercovered{invisible}
\begin{block}{Definition}
\begin{tabular}{lm{2.75in}}
\large $P(a_1, \ldots, a_n)$
&
The \alert{joint probability} of propositions $a_1, \ldots, a_n$ is the degree of belief in the proposition $a_1 \land \ldots \land a_n$
\end{tabular}
\end{block}
\pause
\begin{block}{Example}
$
\begin{array}{lll}
\lefteqn{P(\textit{DieRoll}_1 = 4, \textit{DieRoll}_2 = 6)} \\
& = & P(\textit{DieRoll}_1 = 4 \land \textit{DieRoll}_2 = 6) \\
& = & \pause 1/36 \\
\end{array}
$
\end{block}
\end{frame}

\begin{frame}{Conditional Probability}
\setbeamercovered{invisible}
\begin{block}{Definition}
\begin{tabular}{lm{3.4in}@{}}
\large $P(a|b)$
&
The \alert{posterior} or \alert{conditional probability} of a proposition $a$ given a proposition $b$ is the degree of belief in $a$, \emph{given that we know only $b$}
\end{tabular}
\end{block}
\pause
\begin{block}{Examples}
$
\begin{array}{lll}
P(\textit{Card} = \mbox{A}\spadesuit|\textit{CardSuit} = \spadesuit) & = & \pause 1/13 \\
\pause
P(\textit{DieRoll}_2 = 5|\textit{DieRoll}_1 = 5) & = & \pause 1/6 \\
\end{array}
$
\end{block}
\end{frame}

\begin{frame}{Prior, Joint or Conditional?}
\begin{enumerate}
\item Probability of a having a cavity? \\
\pause Prior, $P(\textit{Cavity}=\textit{true})$
\pause
\item Probability of it being warm and cloudy? \\
\pause Joint, $P(\textit{Temp}=\textit{warm}, \textit{Sky}=\textit{cloudy})$
\pause
\item Probability of car being stolen? \\
\pause Prior, $P(\textit{CarStolen}=\textit{true})$
\pause
\item Probability of car being stolen and being in Alabama? \\
\pause Joint, $P(\textit{CarStolen}=\textit{true}, \textit{InAlabama}=\textit{true})$
\pause
\item The car is in Alabama. Probability of car being stolen? \\
\pause Conditional, $P(\textit{CarStolen}=\textit{true}|\textit{InAlabama}=\textit{true})$
\pause
\item It's cloudy. Probability of it being warm? \\
\pause Conditional, $P(\textit{Temp}=\textit{warm}| \textit{Sky}=\textit{cloudy})$
\end{enumerate}
\end{frame}

\begin{frame}{Relation between Joint and Conditional}
\setbeamercovered{invisible}
\begin{block}{Product Rule}
$P(a \land b) = P(a|b)P(b)$ \hfill or \hfill $P(a|b) = P(a \land b)/P(b)$
\end{block}
\pause
\begin{block}{Intuition}
To have $a \land b$ true, we need $b$ true, and $a$ true given $b$
\end{block}
\pause
\begin{block}{Example}
$
\begin{array}{lll}
P(\mbox{A} \land \spadesuit) & = & \pause 1/52 \\
\pause
P(\mbox{A}|\spadesuit) & = & \pause 1/13 \\
\pause
P(\spadesuit) & = & \pause 1/4  \\
\pause
P(\mbox{A}|\spadesuit)P(\spadesuit) & = & \pause 1/13 \cdot 1/4 = 1/52 \\
\end{array}
$
\end{block}
\end{frame}

\begin{frame}{Chain Rule}
\setbeamercovered{invisible}
\begin{block}{Key Ideas}
\begin{itemize}
\item Repeatedly apply product rule, $P(a,b) = P(a|b)P(b)$
\item Joint probability $\rightarrow$ conditional probabilities
\end{itemize}
\end{block}
\pause
\begin{block}{Example}
$
\begin{array}{lll}
\lefteqn{P(\textit{sunny}, \textit{dry}, \textit{warm})} \\
\pause
& = & P(\textit{sunny}|\textit{dry}, \textit{warm})P(\textit{dry}, \textit{warm})\\
\pause
& = & P(\textit{sunny}|\textit{dry}, \textit{warm})P(\textit{dry}|\textit{warm})P(\textit{warm}) \\
\end{array}
$
\end{block}
\end{frame}


\section{Probability Distributions}


\subsection{Probability Distribution Basics}


\begin{frame}{Probability Distributions}
\setbeamercovered{invisible}
\begin{block}{Definition}
\begin{tabular}{lm{0.8\textwidth}@{}}
\large $\mathbf{P}(X)$
&
The \alert{probability distribution} of a random variable $X$ is a list of probabilities for each domain value
\end{tabular}
\end{block}
\pause
\begin{block}{Example}
$\mathbf{P}(\textit{SkyInBirmingham}) = \langle 210/365, 155/365 \rangle$ means \\[.25em]
$
\begin{array}{llll}
& P(\textit{SkyInBirmingham}=\textit{sunny})  & = & 210/365 \\
& P(\textit{SkyInBirmingham}=\textit{cloudy}) & = & 155/365 \\
\end{array}
$
\end{block}
\pause
\begin{block}{Notation Warning}
\begin{itemize}
\item $P(a)$ or $P(X=a)$ means prior probability
\item $\mathbf{P}(X)$ means probability distribution
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Probability Distributions}
\setbeamercovered{invisible}
\begin{block}{Key Idea}
\begin{tabular}{lm{0.55\textwidth}@{}}
\large $\sum\limits^{d}_{i}{P(X=X_{i})} = 1$
&
The sum of the probabilities for all possible value assignments of the random variable is always 1
\end{tabular}
\end{block}
\pause
\begin{block}{Example}
$
\begin{array}{lll}
\lefteqn{\sum\limits_{x}{P(\textit{Die}=x)}} \\
\pause
& = & P(\textit{Die}=1) + P(\textit{Die}=2) + P(\textit{Die}=3) + \mbox{}\\
&   & P(\textit{Die}=4) + P(\textit{Die}=5) + P(\textit{Die}=6) \\
\pause
& = & \frac{1}{6} + \frac{1}{6} + \frac{1}{6} + \frac{1}{6} + \frac{1}{6} + \frac{1}{6} \\
\pause
& = & 1
\end{array}
$
\end{block}
\end{frame}

\begin{frame}[label=continuous-distributions]{\large Continuous Variable Probability Distributions}
\begin{block}{Definition}
A \alert{probability density function} is a probability distribution over a continuous variable
\end{block}
\begin{block}{Key Idea}
Function assigns a probability to all possible values
\end{block}
\begin{block}{Example}<2->
\begin{tabular}{ l @{\hspace{2em}} r }
\large $P(x) =
\frac{1}{{\only<3->{\color{red}}\sigma}\sqrt{2\pi}}
e^{-\frac{(x - {\only<3->{\color{blue}}\mu})^{2}}{2{\only<3->{\color{red}}\sigma}^{2}}}$ 
&
\uncover<3->{\raisebox{-.5\height}{
\begin{tikzpicture}[domain=0:3]
\draw[->] (0,0) -- (0,1) node[above] {$P(x)$};
\draw[->] (0,0) -- (3,0) node[right] {$x$};
\draw[blue] (1.5,1.1) -- (1.5,-0.1) node[below] {$\mu$};
\draw[red] (1,1.1) -- (1,-0.1) node[below] {$\sigma$};
\draw[red] (2,1.1) -- (2,-0.1) node[below] {$\sigma$};
\draw[smooth] plot (\x,{gauss(1.5,0.5)});
\end{tikzpicture}}}
\end{tabular}
\end{block}
\end{frame}

\begin{frame}{Joint Probability Distributions}
	\begin{block}{Definition}
		\begin{tabular}{@{}lm{0.65\textwidth}@{}}
			\large $\mathbf{P}(X_1, \ldots, X_n)$
			&
			The \alert{joint probability distribution} of random variables $X_1, \ldots, X_n$ is a table of probabilities for each combination of values in the variable domains
		\end{tabular}
	\end{block}
	\pause
	\begin{block}{Example}
		$\mathbf{P}(\textit{Sex},\textit{Smoker}) = \mbox{}$ \\[.25em]
		\tab
		$
		\begin{array}{lcc}
			                           & \textit{Sex}=\textit{male} & \textit{Sex}=\textit{female} \\
			\textit{Smoker}=\textit{true}  & 0.113                  & 0.107 \\
			\textit{Smoker}=\textit{false} & 0.377                  & 0.403 \\
		\end{array}
		$
	\end{block}
\end{frame}


\subsection{Inference from Joint Distributions}


\begin{frame}[label=inference-joint-distributions]{\large Inference with Joint Probability Distributions}
\setbeamercovered{invisible}
\begin{block}{Key Idea}
Sum entries in joint distribution where proposition is true
\end{block}
\pause
\begin{block}{Example: $P(\textit{Sex}=\textit{female} \lor \textit{Smoker}=\textit{false})$}
$
\begin{array}{lcc}
& \textit{Sex}=\textit{male} & \textit{Sex}=\textit{female} \\
\textit{Smoker}=\textit{true}  & 0.113                  & 0.107 \\
\textit{Smoker}=\textit{false} & 0.377                  & 0.403 \\
\end{array}
$
\\[0.5em]
\pause
$
\begin{array}{lll}
\lefteqn{P(\textit{Sex}=\textit{female} \lor \textit{Smoker}=\textit{false})} \\
\pause
& = & P(\textit{Sex}=\textit{male},\textit{Smoker}=\textit{false}) + \mbox{}\\
&   & P(\textit{Sex}=\textit{female},\textit{Smoker}=\textit{true}) + \mbox{}\\
&   & P(\textit{Sex}=\textit{female},\textit{Smoker}=\textit{false})\\
\pause
& = & 0.377 + 0.107 + 0.403 \pause = 0.887
\end{array}
$
\end{block}
\end{frame}

\begin{frame}{Marginalization}
\setbeamercovered{invisible}
\begin{block}{Key Idea}
\begin{tabular}{lm{0.6\textwidth}}
$
\mathbf{P}(Y) = \sum\limits_{z}{\mathbf{P}(Y, z)}
$
&
\alert{Marginalization} removes all variables but $\mathbf{Y}$ by summing over the values of the other variables
\end{tabular}
\end{block}
\pause
\begin{block}{Example: $P(\textit{Sex}=\textit{female})$}
The other variable is $\textit{Smoker}$, so: \\[.5em]
$
\begin{array}{lll}
\lefteqn{P(\textit{Sex}=\textit{female})} \\
\pause & = & P(\textit{Sex}=\textit{female},\textit{Smoker}=\textit{true}) + \mbox{}\\
&   & P(\textit{Sex}=\textit{female},\textit{Smoker}=\textit{false})\\
\pause & = & 0.107 + 0.403 \pause = 0.51
\end{array}
$
\end{block}
\end{frame}

\begin{frame}{Normalization}
\setbeamercovered{invisible}
\begin{block}{Key Idea}
$
\mathbf{P}(Y|z) = \dfrac{\mathbf{P}(Y,z)}{P(z)} = \dfrac{\mathbf{P}(Y,z)}{\sum\limits_{y} P(y, z)} = \alpha\mathbf{P}(Y,z)
$
\end{block}
\pause
\begin{block}{Calculating a Normalizing Constant}
\begin{columns}
\begin{column}{0.35\textwidth}
$\begin{array}{ l l l }
P(\spadesuit,\mbox{A}) & = & \frac{1}{52} \\
P(\clubsuit,\mbox{A}) & = & \frac{1}{52} \\
P(\diamondsuit,\mbox{A}) & = & \frac{1}{52} \\
P(\heartsuit,\mbox{A}) & = & \frac{1}{52}
\end{array}$
\end{column}
\pause
\begin{column}{0.6\textwidth}
$\alpha = \pause \dfrac{1}{\frac{1}{52} + \frac{1}{52} + \frac{1}{52} + \frac{1}{52}} = 13$
\end{column}
\end{columns}
\bigskip
\centering
\pause
$\mathbf{P}(Suit|\mbox{A})
= \langle \frac{13}{52}, \frac{13}{52}, \frac{13}{52}, \frac{13}{52} \rangle
= \langle \frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4} \rangle$
\end{block}
\end{frame}

\begin{frame}[label=inference-exercises]{Inference Exercises}
\setbeamercovered{invisible}
\begin{tabular}{ @{} l m{0.5\textwidth}@{} }
\small
\arraycolsep=0.5em
$
\begin{array}{ @{} l l l l@{} }
\textit{sunny}  & \textit{warm} & \textit{snow} & 0.01 \\
\textit{sunny}  & \textit{warm} & \lnot\textit{snow} & 0.59 \\
\textit{sunny}  & \textit{cold} & \textit{snow} & 0.08 \\
\textit{sunny}  & \textit{cold} & \lnot\textit{snow} & 0.14 \\
\textit{cloudy} & \textit{warm} & \textit{snow} & 0.03 \\
\textit{cloudy} & \textit{warm} & \lnot\textit{snow} & 0.07 \\
\textit{cloudy} & \textit{cold} & \textit{snow} & 0.06 \\
\textit{cloudy} & \textit{cold} & \lnot\textit{snow} & 0.02
\end{array}
$
&
\begin{enumerate}
\item\small $P(\textit{sunny}) = \uncover<2>{0.82}$
\item\small $P(\textit{warm}) = \uncover<2>{0.70}$
\item\small $P(\textit{snow}) = \uncover<2>{0.18}$
\item\small $P(\textit{sunny} \lor \lnot\textit{snow}) = \uncover<2>{0.91}$
\item\small $P(\textit{sunny}|\textit{snow}) = \uncover<2>{0.50}$
\item\small $P(\textit{snow}|\textit{sunny},\textit{cold}) = \uncover<2>{0.36}$
\end{enumerate}
\end{tabular}
\end{frame}

\begin{frame}{Joint Distribution Inference}
\begin{block}{Properties}
Given $n$ random variables with maximum domain size $d$:
\begin{description}[Space Complexity?]
\item[Time Complexity?]  \uncover<2->{$O(d^{n})$}
\item[Space Complexity?] \uncover<3->{$O(d^{n})$}
\end{description}
\end{block}
\begin{block}{Biggest Problem}<4->
How do you fill in a table of $O(d^{n})$ probabilities?
\end{block}
\end{frame}


\section{Using Independence}


\subsection{Independence}


\begin{frame}{Independence}
\begin{block}{Key Ideas}
\begin{itemize}
\item Sometimes no connection exists between variables
\item Such independence determined by world knowledge
\end{itemize}
\end{block}
\pause
\begin{block}{Formal Independence}
$P(a, b) = P(a)P(b)$ \tab or \tab $P(a|b) = P(a)$
\end{block}
\pause
\begin{block}{Examples}
$
\begin{array}{lll}
P(\textit{EyeColor},\textit{Gender}) & = & P(\textit{EyeColor})P(\textit{Gender}) \\
P(\textit{Cavity},\textit{BlazersWon}) & = & P(\textit{Cavity})P(\textit{BlazersWon}) \\
P(\textit{DieRoll}_{1},\textit{DieRoll}_{2}) & = & P(\textit{DieRoll}_{1})P(\textit{DieRoll}_{2}) \\
\end{array}
$
\end{block}
\end{frame}

\begin{frame}{Independence}
\setbeamercovered{invisible}
\begin{block}{Using Independence}
\begin{itemize}
\item No need to store entire joint probability table
\item Can store several smaller independent tables
\end{itemize}
\end{block}
\pause
\begin{block}{Examples}
$
\begin{array}{ll}
& \mbox{\textcolor{uabgreen}{Table Size}} \\
\pause\mathbf{P}(\textit{DieRoll}_{1}, \textit{DieRoll}_{2}) & \pause 6 \cdot 6 = 36 \\
\pause\mathbf{P}(\textit{DieRoll}_{1})\mathbf{P}(\textit{DieRoll}_{2}) & \pause 6 + 6 = 12 \\
\pause\mathbf{P}(\textit{Age}, \textit{Sex}, \textit{BlazersWon}) & \pause 125 \cdot 2 \cdot 2 = 500 \\
\pause\mathbf{P}(\textit{Age}, \textit{Sex})\mathbf{P}(\textit{BlazersWon}) & \pause 125 \cdot 2 + 2 = 252 \\
\end{array}
$
\end{block}
\end{frame}

\begin{frame}{Conditional Independence}
	\begin{block}{Key Idea}
		Two variables can sometimes become independent after the value of a third variable is observed
	\end{block}
	\begin{block}{Definition}
		A random variable $X$ is \alert{conditionally independent} of random variable $Y$ given the random variable $Z$ if: \\
		\tab$\mathbf{P}(X,Y|Z) = \mathbf{P}(X|Z)\mathbf{P}(Y|Z)$ \\
		or \\
		\tab$\mathbf{P}(X|Y,Z) = \mathbf{P}(X|Z)$
	\end{block}
\end{frame}

\begin{frame}{Conditional Independence Example}
\begin{block}{Is $\textit{GrayHair}$ indpendent of $\textit{Bifocals}$?}
\pause
No, we expect the two to often come together, e.g.: \\[.5em]
$P(\textit{gray-hair}, \textit{bifocals}) > P(\textit{gray-hair})P(\textit{bifocals})$
\end{block}
\pause
\begin{block}{Is $\textit{GreyHair}$ independent of $\textit{Bifocals}$ given $\textit{Age}$?}
\pause
Yes, the bifocals add nothing if we know the age, e.g.: \\[.5em]
$P(\textit{gray-hair}|\textit{bifocals},\textit{Age}\!=\!x) = P(\textit{gray-hair}|\textit{Age}\!=\!x)$
\end{block}
\end{frame}

\begin{frame}{Conditional Independence Example}
\begin{block}{Noisy Phone}
\begin{itemize}
\item Adam calls Betty and Charlie and says a number $N_{A}$
\item Betty hears $N_{B}$ and Charlie hears $N_{C}$
\end{itemize}
\end{block}
\pause
\begin{block}{Are $N_{B}$ and $N_{C}$ independent?}
\pause
No, we expect the numbers to be similar, e.g.: \\[.5em]
$P(N_{B}=1, N_{C}=1) > P(N_{B}=1)P(N_{C}=1)$
\end{block}
\pause
\begin{block}{Are $N_{B}$ and $N_{C}$ independent given $N_{A}$?}
\pause
Yes, Betty's number adds nothing if we know Adam's, e.g.: \\[.5em]
$P(N_{C}=1|N_{B}=2,N_{A}=1) = P(N_{C}=1|N_{A}=1)$
\end{block}
\end{frame}


\subsection{Bayes' Rule}


\begin{frame}{Bayes' Rule}
\begin{block}{Key Idea}
Swap the conditioned and conditioning variables
\end{block}
\begin{block}{Bayes' Rule}
\tab $P(b|a) = \dfrac{P(a|b)P(b)}{P(a)}$
\end{block}
\pause
\begin{block}{Derivation}
$
\begin{array}{llll}
P(b|a)
& = & \dfrac{P(a \land b)} {P(a)} & \mbox{Definition} \\[1em]
& = & \dfrac{P(a|b)P(b)} {P(a)} & \mbox{Product Rule}
\end{array}
$
\end{block}
\end{frame}

\begin{frame}{Why Use Bayes' Rule?}
\begin{block}{Making Diagnoses Based on Causal Knowledge}
$
P(\textit{Cause}|\textit{Effect}) =
\dfrac{P(\textit{Effect}|\textit{Cause})P(\textit{Cause})}{P(\textit{Effect})}
$
\end{block}
\pause
\begin{block}{Example}
{\small$
P(\textit{meningitis}|\textit{stiff{\scriptsize -}neck}) = 
\dfrac{P(\textit{stiff{\scriptsize -}neck}|\textit{meningitis})P(\textit{meningitis})}{P(\textit{stiff{\scriptsize -}neck})}
$}
\\[1em]
\pause
In an epidemic, where $P(\textit{meningitis})$ rises:
\begin{description}[+Bayes]
\item[+Bayes] $P(\textit{meningitis}|\textit{stiff-neck})$ rises proportionally
\item[-Bayes] Collect data, re-estimate $P(\textit{meningitis}|\textit{stiff-neck})$
\end{description}
\end{block}
\end{frame}

\begin{frame}{Bayes' Rule \& Conditional Independence}
\setbeamercovered{invisible}
	\begin{block}{Deriving More Manageable Models}
		$
		\begin{array}{lll}
		\lefteqn{P(\textit{Cavity}|\textit{toothache}, \textit{catch})} \\
		\pause
		& = & \alpha\displaystyle P(\textit{toothache}, \textit{catch}|\textit{Cavity})
		                          P(\textit{Cavity}) \\
		\pause
		& = & \alpha\displaystyle P(\textit{toothache}|\textit{Cavity})
		                          P(\textit{catch}|\textit{Cavity})
		                          P(\textit{Cavity}) \\
		\end{array}
		$
	\end{block}
	\pause
	\begin{block}{Naive Bayes Models}
		\small
		$
		P(\textit{Cause},\textit{Effect}_{1},\ldots,\textit{Effect}_{n})
		 = P(\textit{Cause})\prod_{i}{P(\textit{Effect}_{i}|\textit{Cause})}
		$
		\normalsize
		\begin{itemize}
			\item All effects conditionally independent given cause
			\item Common class of machine learning models
		\end{itemize}
	\end{block}
\end{frame}

\subsection{Wumpus World Example}

\begin{frame}[label=wumpus-world-example]{Wumpus World Example}
\begin{columns}[T]
\begin{column}{0.52\textwidth}
\small
\begin{wumpusgrid}{4}{0.24\textwidth}
\foreach \x in {0,...,3} \foreach \y in {0,...,3} {%
\pgfmathtruncatemacro{\xone}{1+\x}%
\pgfmathtruncatemacro{\yone}{1+\y}%
\wumpusbottom{1-}{\x}{\y}{\scriptsize (\xone,\yone)}
}
\wumpusknowledge{1-}{0}{0}{$\scriptstyle\neg P$}
\wumpusknowledge{1-}{1}{0}{$\scriptstyle\neg P$}
\wumpusknowledge{1-}{0}{1}{$\scriptstyle\neg P$}
\wumpuspercept{1-}{1}{0}{$\scriptstyle B$}
\wumpuspercept{1-}{0}{1}{$\scriptstyle B$}
\end{wumpusgrid}
\end{column}
\pause
\begin{column}{0.45\textwidth}
\begin{block}{Query}
$P_{1,3}$?
\end{block}
\pause
\begin{block}{Brute Force Approach}
\begin{itemize}
\item Calculate full joint distribution
\item Sum all rows where:
      $p_{1,3}$, $\lnot p_{1,1}$, $\lnot p_{1,2}$, $\lnot p_{2,1}$, $b_{1,2}$, $b_{2,1}$
\pause\item Result is $P(p_{1,3})$
\end{itemize}
\end{block}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Wumpus World Brute Force}
\begin{columns}[T]
\begin{column}{0.47\textwidth}
\small
\begin{wumpusgrid}{4}{0.24\textwidth}
\foreach \x in {0,...,3} \foreach \y in {0,...,3} {%
\pgfmathtruncatemacro{\xone}{1+\x}%
\pgfmathtruncatemacro{\yone}{1+\y}%
\wumpusbottom{1-}{\x}{\y}{\scriptsize (\xone,\yone)}
}
\end{wumpusgrid}
\end{column}
\begin{column}{0.50\textwidth}
\begin{block}{Full Joint Distribution}
\pause
\small $\mathbf{P}(P_{11},\ldots,P_{44},B_{11},\ldots,B_{44})$
\end{block}
\pause
\begin{block}{Total Rows?}
\pause
$2^{16 + 16} = 2^{32}$
\end{block}
\pause
\begin{block}{Rows Selected?}
Assume: \\
$p_{13}, \lnot p_{11}, \lnot p_{12}, \lnot p_{21}, b_{12}, b_{21}$
\\[.5em]
\pause
$2^{12 + 14} = 2^{28}$
\end{block}
\pause
There must be a better way!
\end{column}
\end{columns}
\end{frame}

\begin{frame}[label=wumpus-world-with-independence]{Wumpus World with Independence}
\begin{columns}[T]
\begin{column}{0.52\textwidth}
\small
\begin{wumpusgrid}{4}{0.24\textwidth}
\foreach \x in {0,...,3} \foreach \y in {0,...,3} {%
\pgfmathtruncatemacro{\xone}{1+\x}%
\pgfmathtruncatemacro{\yone}{1+\y}%
\wumpusbottom{1-}{\x}{\y}{\scriptsize (\xone,\yone)}
}
\wumpusknowledge{1-}{0}{0}{$\scriptstyle\neg P$}
\wumpusknowledge{1-}{1}{0}{$\scriptstyle\neg P$}
\wumpusknowledge{1-}{0}{1}{$\scriptstyle\neg P$}
\wumpuspercept{1-}{1}{0}{$\scriptstyle B$}
\wumpuspercept{1-}{0}{1}{$\scriptstyle B$}
\wumpustop[blue]{2-}{0}{2}{Query}
\wumpustop[blue]{2-}{1}{1}{Fringe}
\wumpustop[blue]{2-}{2}{0}{Fringe}
\wumpustop[gray]{2-}{0}{3}{Other}
\wumpustop[gray]{2-}{1}{2}{Other}
\wumpustop[gray]{2-}{1}{3}{Other}
\wumpustop[gray]{2-}{2}{1}{Other}
\wumpustop[gray]{2-}{2}{2}{Other}
\wumpustop[gray]{2-}{2}{3}{Other}
\wumpustop[gray]{2-}{3}{0}{Other}
\wumpustop[gray]{2-}{3}{1}{Other}
\wumpustop[gray]{2-}{3}{2}{Other}
\wumpustop[gray]{2-}{3}{3}{Other}
\end{wumpusgrid}
\end{column}
\begin{column}{2.2in}
\begin{block}{Key Idea}
Observations conditionally independent of other squares given neighboring squares
\end{block}
\begin{block}<4->{Goal}
Manipulate equation until we can apply the conditional independence formula
\end{block}
\end{column}
\end{columns}
\medskip
\uncover<3->{$\mathbf{P}(b_{12}, b_{21}|\lnot p_{11}, \lnot p_{12}, \lnot p_{21}, P_{13}, P_{\textit{\scriptsize fringe}}, P_{\textit{\scriptsize other}}) = \mathbf{P}(b_{12}, b_{21}|\lnot p_{11}, \lnot p_{12}, \lnot p_{21}, P_{13}, P_{\textit{\scriptsize fringe}})$}
\end{frame}

\begin{frame}{Wumpus World Equation Manipulation}
\setbeamercovered{invisible}
	\footnotesize
	$
	\begin{array}{lll}
	\lefteqn{P(p_{13}|\lnot p_{11}, \lnot p_{12}, \lnot p_{21}, b_{12}, b_{21})}
	\\
	\pause
	& = & \alpha
	      P(p_{13},
	        \lnot p_{11},
	        \lnot p_{12},
	        \lnot p_{21},
	        b_{12}, b_{21})
	\\
	\pause
	& = & \alpha
	      P(p_{13},
	        p_{\textit{\tiny known}},
	        b_{12}, b_{21})
	\\
	\pause
	& = & \alpha
	      \sum\limits_{\textit{\tiny fringe} + \textit{\tiny other}}
	      P(p_{13},
	        p_{\textit{\tiny known}},
	        p_{\textit{\tiny fringe} + \textit{\tiny other}},
	        b_{12}, b_{21})
	\\
	\pause
	& = & \alpha
	      \sum\limits_{\textit{\tiny fringe}}
	      \sum\limits_{\textit{\tiny other}}
	      P(p_{13},
	        p_{\textit{\tiny known}},
	        p_{\textit{\tiny fringe}},
	        p_{\textit{\tiny other}},
	        b_{12}, b_{21})
	\\
	\pause
	& = & \alpha
	      \sum\limits_{\textit{\tiny fringe}}
	      \sum\limits_{\textit{\tiny other}}
	      P(b_{12}, b_{21}|
	        p_{13},
	        p_{\textit{\tiny known}},
	        p_{\textit{\tiny fringe}},
	        p_{\textit{\tiny other}})
	      P(p_{13},
	        p_{\textit{\tiny known}},
	        p_{\textit{\tiny fringe}},
	        p_{\textit{\tiny other}})
	\\
	\pause
	& = & \alpha
	      \sum\limits_{\textit{\tiny fringe}}
	      \sum\limits_{\textit{\tiny other}}
	      P(b_{12}, b_{21}|
	        p_{13},
	        p_{\textit{\tiny known}},
	        p_{\textit{\tiny fringe}})
	      P(p_{13},
	        p_{\textit{\tiny known}},
	        p_{\textit{\tiny fringe}},
	        p_{\textit{\tiny other}})
	\\
	\pause
	& = & \alpha
	      \sum\limits_{\textit{\tiny fringe}}
	      \sum\limits_{\textit{\tiny other}}
	      P(b_{12}, b_{21}|
	        p_{13},
	        p_{\textit{\tiny known}},
	        p_{\textit{\tiny fringe}})
	      P(p_{13})
	      P(p_{\textit{\tiny known}})
	      P(p_{\textit{\tiny fringe}})
	      P(p_{\textit{\tiny other}})
	\\
	\pause
	& = & \alpha
	      \sum\limits_{\textit{\tiny fringe}}
	      P(b_{12}, b_{21}|
	        p_{13},
	        p_{\textit{\tiny known}},
	        p_{\textit{\tiny fringe}})
	      P(p_{13})
	      P(p_{\textit{\tiny known}})
	      P(p_{\textit{\tiny fringe}})
	      \sum\limits_{\textit{\tiny other}}
	      P(p_{\textit{\tiny other}})
	\\
	\pause
	& = & \alpha
	      \sum\limits_{\textit{\tiny fringe}}
	      P(b_{12}, b_{21}|
	        p_{13},
	        p_{\textit{\tiny known}},
	        p_{\textit{\tiny fringe}})
	      P(p_{13})
	      P(p_{\textit{\tiny known}})
	      P(p_{\textit{\tiny fringe}})
	\\
	\pause
	& = & \alpha
	      P(p_{13})
	      P(p_{\textit{\tiny known}})
	      \sum\limits_{\textit{\tiny fringe}}
	      P(b_{12}, b_{21}|
	        p_{13},
	        p_{\textit{\tiny known}},
	        p_{\textit{\tiny fringe}})
	      P(p_{\textit{\tiny fringe}})
	\end{array}
	$
\end{frame}

\begin{frame}{Wumpus World Probability Calculation}
	\begin{block}{Goal: Calculate the Summation}
		$
		\sum\limits_{p_{22}, p_{31}}
	  P(b_{12}, b_{21}|
	    p_{13},
	    \lnot p_{11},
	    \lnot p_{12},
	    \lnot p_{21},
	    P_{22},
	    P_{31})
	  P(P_{22},
	    P_{31})
		$
	\end{block}
	\pause
	\begin{block}{Key Ideas}
		\begin{itemize}
			\item First term is 1 if observed breezes are beside pits
			\item First term is 0 if observed breezes are not beside pits
			\pause
			\item So find worlds consistent with observations
			\item And sum $P(P_{22}, P_{31})$ over these worlds
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}[label=wumpus-world-probability-walkthrough]{Wumpus World Probability Calculation}
\setbeamercovered{invisible}
\begin{block}{Worlds Consistent with Observations}
\setlength{\tabcolsep}{0.15em}
\begin{tabular}{@{} c c c c c @{}}
\tinywumpusworldexample{P}{P}{P}
&
\tinywumpusworldexample{P}{P}{\lnot P}
&
\tinywumpusworldexample{P}{\lnot P}{P}
&
\tinywumpusworldexample{\lnot P}{P}{P}
&
\tinywumpusworldexample{\lnot P}{P}{\lnot P}
\\
\uncover<3->{\scriptsize $.2\times.2=.04$}
&
\uncover<4->{\scriptsize $.2\times.8=.16$}
&
\uncover<5->{\scriptsize $.8\times.2=.16$}
&
\uncover<6->{\scriptsize $.2\times.2=.04$}
&
\uncover<7->{\scriptsize $.2\times.8=.16$}
\end{tabular}\\[0.5em]
\uncover<2->{Calculating $P(P_{22}, P_{31})$, assuming pit probability is 0.2}
\end{block}
\begin{block}<8->{Final Calculation}
$\sum\limits_{p_{22}, p_{31}}P(b_{12}, b_{21}|p_{13},\ldots)P(P_{22},P_{31}) = 0.04 + 0.16 + 0.16$\\
$\sum\limits_{p_{22}, p_{31}}P(b_{12}, b_{21}|\lnot p_{13},\ldots)P(P_{22},P_{31}) = 0.04 + 0.16$
\end{block}
\end{frame}

\begin{frame}{Wumpus World Probability Calculation}
\setbeamercovered{invisible}
	$
	\begin{array}{lll}
	\lefteqn{P(p_{13}|\lnot p_{11}, \lnot p_{12}, \lnot p_{21}, b_{12}, b_{21})}
	\\
	\pause
	& = &
	\alpha
	P(p_{13})
	P(\lnot p_{11},\lnot p_{12},\lnot p_{21})
	\sum\ldots p_{13}\ldots
	\\
	\pause
	& = &
	\alpha \times
  0.2 \times
	P(\lnot p_{11},\lnot p_{12},\lnot p_{21})
	\sum\ldots p_{13}\ldots
  \\
  \pause
  & = &
  \alpha \times
  0.2 \times
  0.8 \times 0.8 \times 0.8 \times
  \sum\ldots p_{13}\ldots
  \\
  \pause
  & = &
  \alpha \times
  0.2 \times
  0.8 \times 0.8 \times 0.8 \times
  (0.04 + 0.16 + 0.16)
  \\
  \pause
  & = & 0.036864\alpha
  \\[0.5em]
  \pause
	\lefteqn{P(\lnot p_{13}|\lnot p_{11}, \lnot p_{12}, \lnot p_{21}, b_{12}, b_{21})}
	\\
	\pause
	& = & 
	\alpha
	P(\lnot p_{13})
	P(\lnot p_{11},\lnot p_{12},\lnot p_{21})
	\sum\ldots \lnot p_{13}\ldots
  \\
  \pause
  & = &
  \alpha \times
  0.8 \times
  0.8 \times 0.8 \times 0.8 \times
  (0.04 + 0.16)
  \\
  \pause
  & = & 0.08192\alpha
  \\[0.5em]
  \pause
  \lefteqn{P(p_{13}|\lnot p_{11}, \lnot p_{12}, \lnot p_{21}, b_{12}, b_{21})}
  \\
  \pause
  & = & 0.036864 / (0.036864 + 0.08192) \approx 0.31
  \end{array}
  $
\end{frame}

\part{Key Ideas}
\begin{frame}{Probability Rules}
\setbeamercovered{invisible}
	\begin{columns}
		\begin{column}{0.47\textwidth}
			\begin{block}{Product Rule}
				$P(a,b) = P(a|b)P(b)$
			\end{block}
			\begin{block}{Independence}
				$P(a,b) = P(a)P(b)$
			\end{block}
			\begin{block}{Conditional Independence}
				$P(a,b|c) = P(a|c)P(b|c)$ \\
				$P(a|b,c) = P(a|c)$ \\
			\end{block}
		\end{column}
		\pause
		\begin{column}{0.5\textwidth}
			\small
			Given: 
			\begin{tabular}[t]{l}
				$w$ indep. $x,y,z$ \\
				$x$ indep. $z$ given $y$
			\end{tabular}
			\\ \smallskip
			Show:
			\\ \smallskip
			$
			\begin{array}{@{}l@{}}
				P(w,x,y,z) = P(w)P(x|y)P(z,y)
			\end{array}
			$
			\\ \bigskip
			\pause
			Proof:
			\\ \smallskip
			$
			\begin{array}{@{}l@{}}
			\lefteqn{P(w,x,y,z)} \\
			\pause = P(w)P(x,y,z) \\
			\pause = P(w)P(x,z|y)P(y) \\
			\pause = P(w)P(x|y)P(z|y)P(y) \\
			\pause = P(w)P(x|y)P(z,y) \\
			\end{array}
			$
		\end{column}
	\end{columns}
\end{frame}
\begin{frame}{Probability Rule Exercises}
	\begin{columns}
		\begin{column}{0.47\textwidth}
			\begin{block}{Product Rule}
				$P(a,b) = P(a|b)P(b)$
			\end{block}
			\begin{block}{Independence}
				$P(a,b) = P(a)P(b)$
			\end{block}
			\begin{block}{Conditional Independence}
				$P(a,b|c) = P(a|c)P(b|c)$ \\
				$P(a|b,c) = P(a|c)$ \\
			\end{block}
		\end{column}
		\begin{column}{0.5\textwidth}
			\small
			Given: \\
			\begin{tabular}[t]{l}
				$z$ indep. $w,x,y$ \\
				$w$ indep. $x$ given $y$
			\end{tabular}
			\\ \smallskip
			Show:
			\\ \smallskip
			$
			\begin{array}{l@{}}
				P(w,z|x,y) = P(w|y)P(z)
			\end{array}
			$
			\\ \bigskip
			Given: \\
			\begin{tabular}[t]{l}
				$w$ indep. $y,z$ given $x$ \\
				$x$ indep. $z$ given $y$ \\
				$y$ indep. $z$
			\end{tabular}
			\\ \smallskip
			Show:
			\\ \smallskip
			\footnotesize
			$
			\begin{array}{l@{}}
				P(w,x,y,z) = P(w|x)P(x|y)P(y)P(z)
			\end{array}
			$
		\end{column}
	\end{columns}
\end{frame}
\begin{frame}{Normalization Rules}
\setbeamercovered{invisible}
	\begin{columns}
		\begin{column}{0.47\textwidth}
			\begin{block}{Complements}
				$P(\lnot a) + P(a) = 1$
			\end{block}
			\begin{block}{Summing Out}
				$P(a, b) + P(\lnot a, b) = P(b)$
			\end{block}
			\begin{block}{Distributions}
				$\sum\limits^{d}_{i}{P(X=x_{i})} = 1$
			\end{block}
		\end{column}
		\pause
		\begin{column}{0.51\textwidth}
			\small
			Given: $\mathbf{P}(X|y,z) = \alpha\langle 0.1, 0.3 \rangle$
			\\ \smallskip
			Show: $\mathbf{P}(X|y,z) = \langle 0.25, 0.75 \rangle$
			\\ \bigskip
			\pause
			$
			\begin{array}{@{}r@{\hspace{.2em}}l@{\hspace{.2em}}l@{}}
			P(y,z) & = & P(x,y,z) + P(\lnot x,y,z) \\[.2em]
			\pause
			1      & = & \displaystyle\frac{P(x,y,z) + P(\lnot x,y,c)}{P(y, z)} \\[.8em]
			\pause
			1      & = & \displaystyle\frac{P(x,y,z)}{P(y, z)} + \frac{P(\lnot x,y,z)}{P(y, z)} \\[.8em]
			\pause
			\alert<10->{1} & = & \alert<10->{P(x|y,z) + P(\lnot x|y,z)} \\[.2em]
			\pause
			1      & = & 0.1\alpha + 0.3\alpha \\[.2em]
			\pause
			1      & = & 0.4\alpha \\[.2em]
			\pause
			2.5    & = & \alpha
			\end{array}
			$
		\end{column}
	\end{columns}
\end{frame}
\begin{frame}{Normalization Rule Practice}
	\begin{columns}
		\begin{column}{0.47\textwidth}
			\begin{block}{Complements}
				$P(\lnot a) + P(a) = 1$ \\
				$P(a|b) + P(\lnot a|b) = 1$
			\end{block}
			\begin{block}{Summing Out}
				$P(a, b) + P(\lnot a, b) = P(b)$
			\end{block}
			\begin{block}{Distributions}
				$\sum\limits^{d}_{i}{P(X=x_{i})} = 1$
			\end{block}
		\end{column}
		\begin{column}{0.51\textwidth}
			\small
			Given:
			\\ \smallskip
			$
			\begin{array}{lll}
				P(y|x)       & = & 0.4 \\
				P(y|\lnot x) & = & 0.9 \\
				P(x)         & = & 0.2 \\
				P(\lnot x)   & = & 0.8
			\end{array}
			$
			\\ \bigskip
			Show: $\mathbf{P}(X|y) = \langle 0.1, 0.9 \rangle$
		\end{column}
	\end{columns}
\end{frame}
\begin{frame}{Key Ideas}
	\begin{block}{Probability Measures Belief}
		$P(\textit{false}) = 0$ \tab $P(\textit{true}) = 1$ \tab $\sum\limits_{x}P(X=x) = 1$
	\end{block}
	\begin{block}{Types of probabilities}
		\begin{itemize}
			\item Prior, $P(X=x)$
			\item Joint, $P(X=x,Y=y)$
			\item Conditional, $P(X=x|Y=y)$
		\end{itemize}
	\end{block}
	\begin{block}{Inference}
		\begin{itemize}
			\item Sums over full joint distribution
			\item Conditional independence + product and Bayes' rule
		\end{itemize}
	\end{block}
\end{frame}

\end{document}


