% !TEX TS-program = pdflatexmk

\documentclass[14pt]{beamer}
\input{../slides-preamble}

\title{Learning Probabilistic Models}
\date[]{10 Apr 2014}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Outline}
\tableofcontents
\end{frame}

\section{Naive Bayes}

\subsection{Models}

\begin{frame}{Naive Bayes Networks}
\begin{columns}[t]
\begin{column}{2in}
\begin{block}{Bayesian Network}
Represents all variable dependence relations
\end{block}
\begin{center}
\includegraphics[scale=.75]{stolen-bayes-net}
\end{center}
\end{column}
\pause
\begin{column}{2in}
\begin{block}{Naive Bayes Network}
Assumes features are conditionally independent given the class variable
\end{block}
\begin{center}
\includegraphics[scale=.75]{stolen-naive-bayes}
\end{center}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Naive Bayes Models}
\begin{block}{Given class $C$ and features $F_1,\ldots,F_n$}
$
\begin{array}{llll}
\lefteqn{\mathbf{P}(C|F_1,F_2,\ldots,F_n)} \\
& = & \pause\alpha\mathbf{P}(F_1,F_2,\ldots,F_n|C)\mathbf{P}(C) & \mbox{Bayes' Rule} \\
& = & \pause\alpha\mathbf{P}(F_1|C)\mathbf{P}(F_2|C)\ldots\mathbf{P}(F_n|C)\mathbf{P}(C) & \mbox{Naive Bayes}
\end{array}
$
\end{block}
\pause
\vspace{-1pt}
\begin{block}{Naive Bayes models}
$\mathbf{P}(C|F_1,F_2,\ldots,F_n) = \alpha\mathbf{P}(C)\prod\limits_{i=1}^{n}\mathbf{P}(F_{i}|C)$
\end{block}
\pause
\vspace{-1pt}
\begin{block}{Training models}
\begin{itemize}
\item Find the probability of each class
\item Find the probability of each feature given the class
\end{itemize}
\end{block}
\end{frame}

\subsection{Examples}

\begin{frame}{Naive Bayes Classification}
\centering
\begin{tabular}[t]{lll|l}
Origin?  & Color? & Type?  & Stolen? \\
\hline
import   & red    & sports & yes \\
import   & red    & sports & yes \\
domestic & white  & sports & yes \\
domestic & red    & van    & yes \\
import   & red    & van    & no \\
domestic & white  & sports & no \\
\end{tabular}

\pause
\begin{block}{A domestic red sports car}
\small
$
\begin{array}{@{}l@{\hspace{.1em}}l@{\hspace{.1em}}l@{\hspace{.1em}}l@{\hspace{.1em}}l@{\hspace{.1em}}l@{\hspace{.1em}}l@{}}
\pause
P(y|d,r,s) & = & \pause\alpha P(y)P(d|y)P(r|y)P(s|y) 
           & = & \pause\alpha\cdot\pause\frac{2}{3}\cdot\pause\frac{1}{2}\cdot\pause\frac{3}{4}\cdot\pause\frac{3}{4}
           & = & \pause\frac{3}{16}\alpha
\\
\pause
P(n|d,r,s) & = & \pause\alpha P(n)P(d|n)P(r|n)P(s|n)
           & = & \pause\alpha\cdot\frac{1}{3}\cdot\frac{1}{2}\cdot\frac{1}{2}\cdot\frac{1}{2}
           & = & \pause\frac{1}{24}\alpha
\end{array}
$
\normalsize
\medskip

\pause
Predict stolen?
\pause
\alert{yes}
\hfill
\pause
At what probability?
\pause
$\alert{\frac{9}{11}} = \frac{\frac{3}{16}}{\frac{3}{16} + \frac{1}{24}}$
\end{block}
\end{frame}

\begin{frame}{Naive Bayes Exercise}
\begin{center}
\begin{tabular}[t]{cc|c}
Ends with -ed  & Initial Capital  & Part of Speech \\
\hline
no             & no               & noun \\
no             & yes              & noun \\
no             & yes              & noun \\
yes            & no               & noun \\
yes            & yes              & noun \\
no             & no               & verb \\
yes            & no               & verb \\
yes            & yes              & verb \\
\end{tabular}

\bigskip
Assign part of speech tags:
\tab\tab
\begin{tabular}[t]{cc}
John            & tripped \\
\visible<2->{
noun            & verb    \\
$\frac{27}{32}$ & $\frac{5}{8}$ \\
%.84375          & .625 \\
}
\end{tabular}
% John
% P(POS=n | ED=n , CAP=y)
% = \alpha P(POS=n) P(ED=n | POS=n) P(CAP=y | POS=n)
% = \alpha \frac{5}{8} \cdot \frac{3}{5} \cdot \frac{3}{5}
% = \alpha \frac{9}{40}
% P(POS=v | ED=n , CAP=y)
% = \alpha P(POS=v) P(ED=n | POS=v) P(CAP=y | POS=v)
% = \alpha \frac{3}{8} \cdot \frac{1}{3} \cdot \frac{1}{3}
% = \alpha \frac{1}{24}
% \alpha = \frac{15}{4}
% P(POS | ED=n , CAP=y) = \langle \frac{27}{32}, \frac{5}{32} \rangle
% tripped
% P(POS=n | ED=y , CAP=n)
% = \alpha P(POS=n) P(ED=y | POS=n) P(CAP=n | POS=n)
% = \alpha \frac{5}{8} \cdot \frac{2}{5} \cdot \frac{2}{5}
% = \alpha \frac{1}{10}
% P(POS=v | ED=y , CAP=n)
% = \alpha P(POS=v) P(ED=y | POS=v) P(CAP=n | POS=v)
% = \alpha \frac{3}{8} \cdot \frac{2}{3} \cdot \frac{2}{3}
% = \alpha \frac{1}{6}
% \alpha = \frac{15}{4}
% P(POS | ED=y , CAP=n) = \langle \frac{3}{8}, \frac{5}{8} \rangle
\end{center}
\end{frame}

\subsection{Properties}

\begin{frame}{Naive Bayes Properties}
\begin{block}{Naive Bayes assumption is hardly ever true}
\begin{itemize}
\item Probability estimates of Naive Bayes are poor
\item Classification decisions are often surprisingly good
\end{itemize}
\end{block}
\pause
\begin{block}{Empirical Observations}
\begin{itemize}
\item Works best when many equally important features
\item Somewhat robust to noise (uninformative) features
\item Training and classification are typically fast
\end{itemize}
\end{block}
\end{frame}

\part{Key Ideas}
\begin{frame}{Key Ideas}
\begin{block}{Naive Bayes}
Assume features conditionally independent given class
\end{block}
\end{frame}

\end{document}


